{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c0f2513d31234ab8a767903738cbaf56":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9af5182805f406b95fee1e59c437804","IPY_MODEL_393e3658a93e426086a87c81c26dcf10","IPY_MODEL_3328bc9e4b03414cb321582b237ca1be"],"layout":"IPY_MODEL_a16159da9686464f916f9e8a1f78a5b4"}},"d9af5182805f406b95fee1e59c437804":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6595dee8e79c41a2b8529082487bb6e2","placeholder":"​","style":"IPY_MODEL_5aef64b40e0d44fd82394d38e156d2fa","value":"Sanity Checking DataLoader 0:   0%"}},"393e3658a93e426086a87c81c26dcf10":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1182953b470541a29c7a087a2b8bb049","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f46560f822e249a2b6a2f1d752decf75","value":0}},"3328bc9e4b03414cb321582b237ca1be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_016bc0548ba74b0f9d966a66ba2f975a","placeholder":"​","style":"IPY_MODEL_a07378dbaaa14b16a33346f9b6e13f27","value":" 0/2 [00:00&lt;?, ?it/s]"}},"a16159da9686464f916f9e8a1f78a5b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"6595dee8e79c41a2b8529082487bb6e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5aef64b40e0d44fd82394d38e156d2fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1182953b470541a29c7a087a2b8bb049":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f46560f822e249a2b6a2f1d752decf75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"016bc0548ba74b0f9d966a66ba2f975a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a07378dbaaa14b16a33346f9b6e13f27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Désinstaller les versions existantes (si nécessaire)\n!pip uninstall torch-sparse torch-scatter torch-cluster torch-spline-conv torch-geometric -y\n!pip uninstall torch_geometric_temporal -y\n\n# Installer les versions compatibles pour PyTorch 2.5.1 et CUDA 12.1\n!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n!pip install torch-geometric\n!pip install torch_geometric_temporal","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install torch-geometric-temporal\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\n# import folium\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.metrics import mean_absolute_error\nfrom torch_geometric_temporal.nn.attention import ASTGCN\nfrom  torch_geometric.loader import TemporalDataLoader\nfrom pytorch_lightning.callbacks import EarlyStopping","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/benedekrozemberczki/pytorch_geometric_temporal.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.__version__)  # Version de PyTorch\nprint(torch.version.cuda) # Version de CUDA\nprint(torch.cuda.is_available()) # Vérifie si CUDA est disponible","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gdown\n\n# Remplace 'file_id' par l'ID réel du fichier Google Drive\nfile_id = '1_to5cK66dNLtETnEgu-D9FkW2Bu_C5qi'\n\n# Téléchargement du fichier depuis Google Drive\ngdown.download(f'https://drive.google.com/uc?id={file_id}', 'M30.csv', quiet=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('M30.csv')\ndata.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1740916655779,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"vlREy4S4ng9X","outputId":"1e3e6fe6-6729-4d03-b4c3-3292ed2bf9cc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.info()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1740916655810,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"qZ-eH42Ing9Y","outputId":"3729976a-fa27-4c58-d951-3fb0e4834d89","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"elapsed":472,"status":"ok","timestamp":1740916656284,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"CTSqYuYHng9Z","outputId":"d1d474ec-a155-4c20-b585-ce0a36de0eb2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[data['speed_avg'].isna()]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":78,"status":"ok","timestamp":1740916656364,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"wKMciNGLng9Z","outputId":"b56d89d1-b15c-4522-d70f-258790676c66","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.loc[data['speed_avg'].isna() & (data['volume'] == 0), ['speed_avg', 'occupation']] = 0.0\n\n\n","metadata":{"id":"Neo-y7-eng9a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"elapsed":469,"status":"ok","timestamp":1740916656923,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"C41jk7G7ng9a","outputId":"98e9a2ad-32e4-4bcc-92d9-85340b016e98","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[data['occupation'].isna()]\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":87,"status":"ok","timestamp":1740916657012,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"W0422OYPng9b","outputId":"a4dc363d-f9e6-4849-ddd6-8a71014fde27","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[data['id'] == 1001]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":57,"status":"ok","timestamp":1740916657071,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"8tVdTt_Kng9b","outputId":"255525c7-9c64-45d7-eb8b-b3e188f7c2bb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['occupation'] = data.groupby('id')['occupation'].transform(lambda x: x.fillna(x.mean()))\ndata['speed_avg'] = data.groupby('id')['speed_avg'].transform(lambda x: x.fillna(x.mean()))\n","metadata":{"id":"wb4W086Mng9c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"executionInfo":{"elapsed":509,"status":"ok","timestamp":1742463265391,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"hakcKlbMng9c","outputId":"53dfbb0d-3f1a-4108-8477-b2020ee9c262","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['occupation'].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"executionInfo":{"elapsed":672,"status":"ok","timestamp":1740916659713,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"jFml5Ztdng9d","outputId":"6bf603e7-025d-4117-96eb-766e9f2e4051","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Conversion des dates avec utc=True\ndata['time_bin'] = pd.to_datetime(data['time_bin'], utc=True)\n\n# Extraction des composantes de la date et de l'heure\ndata['Mois'] = data['time_bin'].dt.month\ndata['Jour'] = data['time_bin'].dt.day\ndata['Jour_Semaine'] = data['time_bin'].dt.weekday  # 0 = Lundi, 6 = Dimanche\ndata['Heure'] = data['time_bin'].dt.hour\ndata['Minute'] = data['time_bin'].dt.minute\n\n\n\n\n\n","metadata":{"id":"EBFySrpfng9e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encodage cyclique de l'heure\ndata['Heure_sin'] = np.sin(2 * np.pi * data['Heure'] / 24)\ndata['Heure_cos'] = np.cos(2 * np.pi * data['Heure'] / 24)\n\n# Encodage cyclique du jour du mois\ndata['Jour_sin'] = np.sin(2 * np.pi * data['Jour'] / 31)\ndata['Jour_cos'] = np.cos(2 * np.pi * data['Jour'] / 31)\n\n# Encodage cyclique du jour de la semaine\ndata['Jour_Semaine_sin'] = np.sin(2 * np.pi * data['Jour_Semaine'] / 7)\ndata['Jour_Semaine_cos'] = np.cos(2 * np.pi * data['Jour_Semaine'] / 7)\n","metadata":{"id":"lAOvbq8nng9e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1741377385364,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"HdVWghK8ng9f","outputId":"d979b8f6-25fc-42a5-aef1-febaf50fdde4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=data.drop('type',axis=1)\n","metadata":{"id":"xwySukb3ng9f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_combinations = data[['id','heading', 'lon', 'lat']].drop_duplicates()","metadata":{"id":"a9sJ93sBng9f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_combinations.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n# Tracer les arcs (en rouge)\nplt.figure(figsize=(10, 6))\nplt.plot(unique_combinations[\"lon\"], unique_combinations[\"lat\"], linestyle='-', color='red', linewidth=1, label=\"Arcs\")\n\n# Tracer les nœuds (en bleu)\nplt.scatter(unique_combinations[\"lon\"], unique_combinations[\"lat\"], color='blue', s=50, label=\"Nœuds\")\n\n# Ajouter des labels et une grille\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.title(\"Graphique des nœuds et arcs\")\nplt.legend()\nplt.show()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":374,"status":"ok","timestamp":1740916664751,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"YDBhJ_EQng9g","outputId":"2c4813c5-f7a0-4290-be94-351d69c10128","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['occupation'].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1740916664770,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"TNrBOrv9ng9g","outputId":"7d18f8d1-8507-474c-e24a-ef1f5ed724b6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def changer(val):\n    if 0.0 <= val < 20.0:\n        return \"Fluide\"\n    elif 20.0 <= val < 40.0:\n        return \"Lent\"\n    elif 40.0 <= val < 70.0:\n        return \"Embouteillé\"\n    else:\n        return \"blockée\"\n\ndata[\"congestion_level\"] = data[\"congestion_level\"].apply(changer)\n","metadata":{"id":"qolEoRQnng9h","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['congestion_level'].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1740916665793,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"8w-6QJcMng9h","outputId":"d30a94a8-dec2-4c1f-c3b7-9718bf173dcc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nEncodLabel=LabelEncoder()\ndata['congestion_level']=EncodLabel.fit_transform(data['congestion_level'])\nfor label, encoded in zip(EncodLabel.classes_, range(len(EncodLabel.classes_))):\n    print(f\"Label: {label}, Encoded Value: {encoded}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":755,"status":"ok","timestamp":1742463295908,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"XtUINz3yng9i","outputId":"57a1bdda-5821-4ead-9347-757370f5ea2d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.describe()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":389},"executionInfo":{"elapsed":3386,"status":"ok","timestamp":1740916669953,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"WSLzlDkRng9l","outputId":"b0e02d56-20dd-4843-8b72-e200fad54143","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nvalue_counts = data['congestion_level'].value_counts()\nplt.figure(figsize=(8, 6))\nplt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', startangle=90)\nplt.title('Répartition des niveaux de congestion')\nplt.show()\n","metadata":{"id":"p_e9cIz4ng9l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Histogramme pour la colonne 'volume'\nplt.figure(figsize=(8, 6))\nsns.histplot(data['volume'], kde=True, color='blue')\nplt.title('Distribution de Volume')\nplt.xlabel('Volume')\nplt.ylabel('Fréquence')\nplt.show()\n","metadata":{"id":"IQUdgmq7ng9l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Densité de speed_avg\nplt.figure(figsize=(8, 6))\nsns.kdeplot(data['speed_avg'], shade=True)\nplt.title('Distribution de la vitesse moyenne')\nplt.xlabel('Vitesse moyenne ')\nplt.ylabel('Densité')\nplt.show()","metadata":{"id":"PwUTlQ6xng9o","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Courbe de tendance pour 'volume' par mois\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=data, x='Mois', y='volume', hue='congestion_level', marker='o')\nplt.title('Évolution du Volume par Mois')\nplt.xlabel('Mois')\nplt.ylabel('Volume')\nplt.show()\n","metadata":{"id":"gE1PMmJ0ng9o","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Création des graphiques de tendance entre chaque paire de variables\nplt.figure(figsize=(12, 10))\n\n# Tracer volume vs occupation\nplt.subplot(2, 2, 1)\nsns.regplot(x='volume', y='occupation', data=data, line_kws={\"color\": \"red\"})\nplt.title('Volume vs Occupation')\n\n# Tracer volume vs congestion_level\nplt.subplot(2, 2, 2)\nsns.regplot(x='volume', y='congestion_level', data=data, line_kws={\"color\": \"red\"})\nplt.title('Volume vs Congestion Level')\n\n\n# Tracer occupation vs congestion_level\nplt.subplot(2, 2, 4)\nsns.regplot(x='occupation', y='congestion_level', data=data, line_kws={\"color\": \"red\"})\nplt.title('Occupation vs Congestion Level')\n\n# Afficher la figure\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"ndiBHPfang9o","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Sélection des colonnes d'intérêt\ncolumns = ['volume', 'occupation', 'congestion_level','speed_avg']\ncorr_matrix = data[columns].corr()\n\n# Heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Matrice de corrélation')\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":545},"executionInfo":{"elapsed":1600,"status":"ok","timestamp":1741112310573,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"oF43L_LRng9o","outputId":"89c6f698-940a-4ac0-89c9-bb0f16739a01","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import folium\nfrom folium.plugins import HeatMap\nfrom IPython.display import display\n\ndef afficher_heatmap(data, jour, mois):\n    # Filtrer les données pour le jour et le mois spécifiés\n    data_filtre = data[(data[\"Jour\"] == jour) & (data[\"Mois\"] == mois)]\n\n    if data_filtre.empty:\n        print(\"Aucune donnée disponible pour cette date.\")\n        return\n\n    # Créer la carte centrée sur la moyenne des latitudes/longitudes\n    m = folium.Map(location=[data_filtre[\"lat\"].mean(), data_filtre[\"lon\"].mean()], zoom_start=12)\n\n    # Ajouter la heatmap\n    heat_data = list(zip(data_filtre[\"lat\"], data_filtre[\"lon\"], data_filtre[\"congestion_level\"]))\n    HeatMap(heat_data).add_to(m)\n\n    # Afficher la carte directement dans Jupyter\n    display(m)\n","metadata":{"id":"MhdtWNZxng9v","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"afficher_heatmap(data, 17, 9)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":865},"executionInfo":{"elapsed":1551,"status":"ok","timestamp":1740916680646,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"udOxErJ6ng9w","outputId":"2dc3abc5-5035-4e0e-9ae9-d0ea7cfe330c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef afficher_embouteillages_par_heure(data, jour_specifique, mois_specifique):\n\n\n    df_jour = data[(data[\"Jour\"] == jour_specifique) & (data[\"Mois\"] == mois_specifique)]\n    df_jour.groupby(\"Heure\").size().plot(kind=\"bar\", color=\"red\", figsize=(10, 5))\n    plt.xlabel(\"Heure de la journée\")\n    plt.ylabel(\"Nombre de véhicule détectés\")\n    plt.title(f\"Répartition des embouteillages par heure ({jour_specifique}/{mois_specifique}/2021)\")\n    plt.xticks(rotation=0)\n    plt.show()\n\n\n","metadata":{"id":"km_K7iDtng9w","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"afficher_embouteillages_par_heure(data, jour_specifique=8, mois_specifique=6)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1740916680688,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"u9dO8ycrng9x","outputId":"5870db0d-d5a0-4fbb-9172-9b98df0d2c93","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n# Choisir les colonnes à normaliser\ncolonnes_a_normaliser = ['volume', 'occupation', 'congestion_level', 'speed_avg','Mois','Jour','Jour_Semaine','Heure','Minute','Heure_sin','Heure_cos']\n\n# Initialiser le MinMaxScaler\nscaler_minmax = MinMaxScaler()\n\n# Appliquer le scaler sur les colonnes sélectionnées\ndata[colonnes_a_normaliser] = scaler_minmax.fit_transform(data[colonnes_a_normaliser])\ndata.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"executionInfo":{"elapsed":1051,"status":"ok","timestamp":1742463309083,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"I2QZ5Holng9x","outputId":"9e8a2415-fdca-4335-abc9-ae3aa548598a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Liste des colonnes à supprimer\ndelete = [\"lat\", \"lon\", \"heading\",\"Jour_sin\", \"Jour_cos\", \"Jour_Semaine_sin\", \"Jour_Semaine_cos\",]\n\n# Suppression des colonnes\ndata_final = data.drop(columns=delete,axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_final.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\ncorr_matrix = data_final.corr()\n\n# Affichage avec une heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Corrélation entre les variables et le trafic\")\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":655},"executionInfo":{"elapsed":5067,"status":"ok","timestamp":1740916686079,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"},"user_tz":-60},"id":"Er3jlpQJng9x","outputId":"28cc7210-1fcd-4a59-8ded-a4c56654597c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Créer un index multi-niveaux\ntime_sensor_index = pd.MultiIndex.from_arrays([data_final['time_bin'].values, data_final['id'].values])\ndata_final_indexed = data_final.set_index(time_sensor_index)\n\n# Pivoter les données\nresult = data_final_indexed[['volume','occupation','congestion_level','speed_avg','Mois', 'Jour', 'Jour_Semaine', 'Heure', 'Minute','Heure_sin', 'Heure_cos']].unstack()\n\n# Convertir en tenseur 3D\ndata_array = result.to_numpy()\nnum_times = data_array.shape[0]\nnum_sensors = len(data_final['id'].unique())\nnum_features = 11\ndata_restructured_f = data_array.reshape(num_times, num_sensors, num_features)\n\n# Sauvegarder les données\nnp.save(\"data_restructured_f.npy\", data_restructured_f)\nprint(\"Données restructurées sauvegardées ✅\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MaKhFiK8Karf","executionInfo":{"status":"ok","timestamp":1742463384453,"user_tz":-60,"elapsed":3921,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"}},"outputId":"1fe3015e-abff-45a3-d60a-b1b595bd4650","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convertir le tableau numpy en DataFrame\ndata_restructured_df = pd.DataFrame(data_restructured_f.reshape(-1, data_restructured_f.shape[-1]), columns=['volume','occupation','congestion_level','speed_avg','Mois', 'Jour', 'Jour_Semaine', 'Heure', 'Minute','Heure_sin', 'Heure_cos'])\n\n# Appliquer l'interpolation\ndata_restructured_interpolated = data_restructured_df.interpolate(method='linear')\n\n# Convertir à nouveau en ndarray si nécessaire\ndata_restructured_final = data_restructured_interpolated.values.reshape(data_restructured_f.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sauvegarder les données\nnp.save(\"data_restructured_f1.npy\", data_restructured_final)\nprint(\"Données restructurées sauvegardées ✅\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_restructured_final.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Paramètres pour la construction du graphe\nDISTANCE_THRESHOLD_KM = 1.0\nADD_SELF_LOOPS = True # Important pour GAT aussi\nEDGE_WEIGHT_SIGMA = 0.5 # Conservé pour info, mais GAT apprendra ses propres poids\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371\n    dLat = math.radians(lat2 - lat1)\n    dLon = math.radians(lon2 - lon1)\n    lat1, lat2 = map(math.radians, [lat1, lat2])\n    a = math.sin(dLat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dLon / 2)**2\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    return R * c\n\ndef build_spatial_graph_for_gat(sensor_locations_df, id_col, lat_col, lon_col, distance_threshold_km, add_self_loops=True):\n    \"\"\"\n    Construit l'index des arêtes pour GAT (basé sur la proximité).\n    Retourne adj_matrix (binaire ou pondérée pour info) et edge_index.\n    \"\"\"\n    locations = sensor_locations_df[[id_col, lat_col, lon_col]].drop_duplicates(subset=[id_col]).set_index(id_col)\n    sensor_ids = locations.index.tolist()\n    num_nodes = len(sensor_ids)\n    sensor_id_to_index = {sensor_id: i for i, sensor_id in enumerate(sensor_ids)}\n\n    print(f\"Building graph connectivity for {num_nodes} sensors...\")\n    coords_rad = np.radians(locations[[lat_col, lon_col]].values)\n    tree = BallTree(coords_rad, metric='haversine')\n    dist_threshold_rad = distance_threshold_km / 6371\n\n    adj_matrix = np.zeros((num_nodes, num_nodes), dtype=np.float32) # Pour info ou debug\n    edge_list = []\n\n    # Correction ici: supprimer le '[0]' à la fin car return_distance=False par défaut\n    indices = tree.query_radius(coords_rad, r=dist_threshold_rad)\n\n    # Maintenant 'indices' est un tableau où chaque élément est un tableau d'indices de voisins\n    for i, neighbors_indices in enumerate(indices):\n        # neighbors_indices est maintenant un tableau (itérable)\n        for j in neighbors_indices:\n            if i == j: continue\n            adj_matrix[i, j] = 1 # Matrice binaire simple\n            adj_matrix[j, i] = 1 # Symétrique\n            # Ajouter la paire (i,j) si elle n'est pas déjà dans edge_list pour éviter doublons GAT\n            # (Bien que set() à la fin gère cela aussi)\n            if (i,j) not in edge_list and (j,i) not in edge_list:\n                 edge_list.append((i, j))\n\n\n    # Gérer les self-loops pour edge_index si GATConv ne le fait pas (souvent mieux de les inclure)\n    if add_self_loops:\n        np.fill_diagonal(adj_matrix, 1.0)\n        for i in range(num_nodes):\n             # Ajouter (i,i) à edge_list si pas déjà présent (pour être sûr)\n             if not any(item == (i, i) for item in edge_list):\n                 edge_list.append((i, i))\n\n    # Convertir en tensor (set() n'est plus nécessaire si on gère les doublons plus haut)\n    # S'assurer qu'il n'y a pas de doublons si on n'a pas géré plus haut\n    # edge_list = list(set(edge_list)) # Optionnel si la logique ci-dessus est bonne\n   \n    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n\n    print(f\"Graph connectivity built with {edge_index.shape[1]} edges (including self-loops if added).\")\n    return adj_matrix, edge_index, sensor_id_to_index\nadj_matrix2, edge_index2, sensor_id_to_index = build_spatial_graph_for_gat(\n    unique_combinations, 'id','lon','lat', DISTANCE_THRESHOLD_KM, ADD_SELF_LOOPS\n)\n\n# Move edge_index to the correct device\nedge_index3 = edge_index2.to(DEVICE) \nedge_index31 = torch.tensor(edge_index3, dtype=torch.long)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom pytorch_lightning import LightningModule, Trainer\n# Paramètres\nnum_timesteps = 12  # Nombre de pas de temps regardés en arrière\nnum_samples = data_restructured_final.shape[0]\n\n# Séparer 80% Train, 10% Validation, 10% Test\ntrain_size = int(0.8 * num_samples)\nval_size = int(0.1 * num_samples)\ntest_size = num_samples - train_size - val_size\n\n\ntrain_data = data_restructured_final[:train_size]\ntemp_data = data_restructured_final[train_size:]\nval_data = temp_data[:val_size]\ntest_data = temp_data[val_size:]","metadata":{"id":"THWuJ7gfYMUU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nfrom torch.utils.data import Dataset, DataLoader\nclass TrafficDataset(Dataset):\n    def __init__(self, data, num_timesteps=12):\n        self.data = torch.tensor(data, dtype=torch.float32)\n        self.num_timesteps = num_timesteps\n\n    def __len__(self):\n        return len(self.data) - self.num_timesteps\n\n    def __getitem__(self, idx):\n        x = self.data[idx : idx + self.num_timesteps]  # (num_timesteps, num_sensors, num_features)\n        y = self.data[idx + self.num_timesteps, :, :4]  # Seulement les 4 premières variables (volume, occupation, congestion_level, speed_avg)\n        return x.permute(1, 2, 0), y  # ASTGCN attend (num_sensors, num_features, num_timesteps)\n\n# Créer les datasets\ntrain_dataset = TrafficDataset(train_data)\nval_dataset = TrafficDataset(val_data)\ntest_dataset = TrafficDataset(test_data)\n\n\n\n\n\n\n\n\n\n","metadata":{"id":"2tRGzKpcYMXd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Créer les DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\nedge_index = np.array(np.nonzero(adj_matrix2))  # Convertir la matrice d'adjacence en indices\nedge_index31 = torch.tensor(edge_index3, dtype=torch.long)\nprint(f\"✅ DataLoaders prêts : Train={len(train_loader)} batches, Val={len(val_loader)}, Test={len(test_loader)}\")\nprint(edge_index)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport os\n\n# Logger TensorBoard\n# from pytorch_lightning.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=7, mode=\"min\")\n# logger = TensorBoardLogger(\"logs\", name=\"astgcn_experiment\")\n# checkpoint_callback = ModelCheckpoint(\n#     dirpath=\"/content/drive/MyDrive/checkpoints8/\",  # Chemin vers Google Drive\n#     filename=\"astgcn-{epoch:02d}-{val_loss:.4f}\",\n#     save_top_k=3,\n#     monitor=\"val_loss\",\n#     mode=\"min\",\n#     every_n_epochs=1,\n# )\n# checkpoint_dir = \"/content/drive/MyDrive/checkpoints8/\"\n# if os.path.exists(checkpoint_dir) and os.listdir(checkpoint_dir):\n#       latest_checkpoint = checkpoint_dir + max(\n#         os.listdir(checkpoint_dir),\n#         key=lambda f: int(f.split('epoch=')[1].split('-')[0])  # Extract epoch number\n#     )\n#       print(f\"Dernier checkpoint trouvé : {latest_checkpoint}\")\n# else:\n#     latest_checkpoint = None\n#     print(\"Aucun checkpoint trouvé. Démarrage d'un nouvel entraînement.\")\n\n# Configuration du Trainer avec TensorBoard\ntrainer = Trainer(\n    max_epochs=20,  # Nombre maximal d'époques\n    callbacks=[early_stopping],  # Callback pour l'early stopping\n    log_every_n_steps=1,  # Logger les métriques à chaque étape\n    check_val_every_n_epoch=1,  # Valider le modèle à chaque époque\n    enable_progress_bar=True,  # Activer la barre de progression\n    enable_model_summary=True,  # Afficher un résumé du modèle\n    accelerator=\"auto\",  # Utiliser automatiquement le GPU si disponible\n    devices=\"auto\",  # Utiliser automatiquement le GPU si disponible\n\n\n)","metadata":{"id":"XmCjsOMf44RO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convertir adj_matrix1 en torch.Tensor\nadj_matrix1_tensor = torch.tensor(adj_matrix1, dtype=torch.float32)\n\n# Générer edge_index à partir de la matrice d'adjacence\nedge_index1 = (adj_matrix1_tensor > 0).nonzero(as_tuple=False).t().contiguous()\n\nprint(\"edge_index1 :\", edge_index1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Shape de edge_index:\", edge_index31.shape)  # Doit être [2, nombre_d_arêtes]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Récupérer un batch\nx_batch, y_batch = next(iter(train_loader))\n\n# Afficher la forme du batch\nprint(\"Forme du batch x:\", x_batch.shape)\nprint(\"Nombre de features:\", x_batch.shape[-2])\n\n# Si vous voulez afficher les données\nprint(\"\\nPremiers éléments du batch:\")\nprint(x_batch[0])  # Affiche le premier exemple du batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pytorch_lightning as pl\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom typing import Dict, List\n\nclass ASTGCNModel(pl.LightningModule):\n    def __init__(self, num_nodes, edge_index, num_features=11, num_timesteps=12, \n                 output_features=4, dropout=0.3, nb_chev_filter=16, nb_time_filter=32):\n        super().__init__()\n        self.num_nodes = num_nodes\n        self.edge_index = edge_index\n        self.output_features = output_features\n        self.variable_names = ['volume', 'occupation', 'congestion_level', 'speed_avg']\n\n        self.astgcn = ASTGCN(\n            nb_block=2,\n            in_channels=num_features,\n            K=3,\n            nb_chev_filter=nb_chev_filter,\n            nb_time_filter=nb_time_filter,\n            time_strides=1,\n            num_for_predict=num_timesteps,\n            len_input=num_timesteps,\n            num_of_vertices=num_nodes,\n        )\n\n        # Projection finale pour obtenir les 4 features\n        self.output_layer = nn.Sequential(\n            nn.Linear(num_timesteps, 64),\n            nn.ReLU(),\n            nn.Linear(64, output_features))\n            \n        self.dropout = nn.Dropout(dropout)\n        \n        # Stockage des métriques globales\n        self.train_metrics = {'loss': [], 'mse': [], 'rmse': [], 'mae': [], 'r2': []}\n        self.val_metrics = {'loss': [], 'mse': [], 'rmse': [], 'mae': [], 'r2': []}\n        \n        # Stockage des métriques par variable\n        self.train_var_metrics = {var: {'mse': [], 'rmse': [], 'mae': [], 'r2': []} \n                                 for var in self.variable_names}\n        self.val_var_metrics = {var: {'mse': [], 'rmse': [], 'mae': [], 'r2': []} \n                               for var in self.variable_names}\n\n    def forward(self, x):\n        x = x.to(self.device)\n        self.edge_index = self.edge_index.to(self.device)\n        \n        # 1. Passage à travers ASTGCN\n        # Sortie shape: [batch, nodes, timesteps]\n        x = self.astgcn(x, self.edge_index)\n        \n        # 2. Projection vers les 4 features\n        # [batch, nodes, timesteps] -> [batch, nodes, 4]\n        x = self.output_layer(x)\n        \n        return x\n\n    def _compute_metrics(self, y_hat, y):\n        \"\"\"Calcule les métriques globales\"\"\"\n        y_hat_np = y_hat.detach().cpu().numpy().flatten()\n        y_np = y.detach().cpu().numpy().flatten()\n        \n        mse = mean_squared_error(y_np, y_hat_np)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(y_np, y_hat_np)\n        r2 = r2_score(y_np, y_hat_np)\n        \n        return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n\n    def _compute_metrics_by_variable(self, y_hat, y):\n        \"\"\"Calcule les métriques pour chaque variable séparément\"\"\"\n        metrics = {}\n        for i, var_name in enumerate(self.variable_names):\n            y_hat_var = y_hat[..., i].flatten().detach().cpu().numpy()\n            y_var = y[..., i].flatten().detach().cpu().numpy()\n            \n            metrics[var_name] = {\n                'mse': mean_squared_error(y_var, y_hat_var),\n                'rmse': np.sqrt(mean_squared_error(y_var, y_hat_var)),\n                'mae': mean_absolute_error(y_var, y_hat_var),\n                'r2': r2_score(y_var, y_hat_var)\n            }\n        return metrics\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x, y = x.to(self.device), y.to(self.device)\n        y_hat = self(x)\n        \n        # Calcul des métriques\n        loss = nn.MSELoss()(y_hat, y)\n        metrics = self._compute_metrics(y_hat, y)\n        var_metrics = self._compute_metrics_by_variable(y_hat, y)\n        \n        # Logging global\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"train_mse\", metrics['mse'], on_step=False, on_epoch=True)\n        self.log(\"train_rmse\", metrics['rmse'], on_step=False, on_epoch=True)\n        self.log(\"train_mae\", metrics['mae'], on_step=False, on_epoch=True)\n        self.log(\"train_r2\", metrics['r2'], on_step=False, on_epoch=True, prog_bar=True)\n        \n        # Logging par variable\n        for var_name in self.variable_names:\n            self.log(f\"train_{var_name}_mse\", var_metrics[var_name]['mse'], on_step=False, on_epoch=True)\n            self.log(f\"train_{var_name}_rmse\", var_metrics[var_name]['rmse'], on_step=False, on_epoch=True)\n            self.log(f\"train_{var_name}_mae\", var_metrics[var_name]['mae'], on_step=False, on_epoch=True)\n            self.log(f\"train_{var_name}_r2\", var_metrics[var_name]['r2'], on_step=False, on_epoch=True)\n        \n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x, y = x.to(self.device), y.to(self.device)\n        y_hat = self(x)\n        \n        # Calcul des métriques\n        loss = nn.MSELoss()(y_hat, y)\n        metrics = self._compute_metrics(y_hat, y)\n        var_metrics = self._compute_metrics_by_variable(y_hat, y)\n        \n        # Logging global\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val_mse\", metrics['mse'], on_step=False, on_epoch=True)\n        self.log(\"val_rmse\", metrics['rmse'], on_step=False, on_epoch=True)\n        self.log(\"val_mae\", metrics['mae'], on_step=False, on_epoch=True)\n        self.log(\"val_r2\", metrics['r2'], on_step=False, on_epoch=True, prog_bar=True)\n        \n        # Logging par variable\n        for var_name in self.variable_names:\n            self.log(f\"val_{var_name}_mse\", var_metrics[var_name]['mse'], on_step=False, on_epoch=True)\n            self.log(f\"val_{var_name}_rmse\", var_metrics[var_name]['rmse'], on_step=False, on_epoch=True)\n            self.log(f\"val_{var_name}_mae\", var_metrics[var_name]['mae'], on_step=False, on_epoch=True)\n            self.log(f\"val_{var_name}_r2\", var_metrics[var_name]['r2'], on_step=False, on_epoch=True)\n        \n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        x, y = x.to(self.device), y.to(self.device)\n        y_hat = self(x)\n        \n        # Calcul des métriques\n        loss = nn.MSELoss()(y_hat, y)\n        metrics = self._compute_metrics(y_hat, y)\n        var_metrics = self._compute_metrics_by_variable(y_hat, y)\n        \n        # Logging global\n        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n        self.log(\"test_mse\", metrics['mse'], on_step=False, on_epoch=True)\n        self.log(\"test_rmse\", metrics['rmse'], on_step=False, on_epoch=True)\n        self.log(\"test_mae\", metrics['mae'], on_step=False, on_epoch=True)\n        self.log(\"test_r2\", metrics['r2'], on_step=False, on_epoch=True)\n        \n        # Logging par variable\n        for var_name in self.variable_names:\n            self.log(f\"test_{var_name}_mse\", var_metrics[var_name]['mse'], on_step=False, on_epoch=True)\n            self.log(f\"test_{var_name}_rmse\", var_metrics[var_name]['rmse'], on_step=False, on_epoch=True)\n            self.log(f\"test_{var_name}_mae\", var_metrics[var_name]['mae'], on_step=False, on_epoch=True)\n            self.log(f\"test_{var_name}_r2\", var_metrics[var_name]['r2'], on_step=False, on_epoch=True)\n        \n        return {'test_loss': loss, 'metrics': metrics, 'var_metrics': var_metrics}\n\n    def on_train_epoch_end(self):\n        # Stockage des métriques globales\n        metrics = self.trainer.callback_metrics\n        epoch = self.current_epoch\n        \n        if 'train_loss' in metrics:\n            self.train_metrics['loss'].append(metrics['train_loss'].item())\n            self.train_metrics['mse'].append(metrics['train_mse'].item())\n            self.train_metrics['rmse'].append(metrics['train_rmse'].item())\n            self.train_metrics['mae'].append(metrics['train_mae'].item())\n            self.train_metrics['r2'].append(metrics['train_r2'].item())\n        \n        # Stockage des métriques par variable\n        for var_name in self.variable_names:\n            self.train_var_metrics[var_name]['mse'].append(metrics[f'train_{var_name}_mse'].item())\n            self.train_var_metrics[var_name]['rmse'].append(metrics[f'train_{var_name}_rmse'].item())\n            self.train_var_metrics[var_name]['mae'].append(metrics[f'train_{var_name}_mae'].item())\n            self.train_var_metrics[var_name]['r2'].append(metrics[f'train_{var_name}_r2'].item())\n        \n        # Affichage\n        print(f\"\\nEpoch {epoch} - Train Metrics:\")\n        print(f\"Global - Loss: {metrics['train_loss'].item():.4f} | MSE: {metrics['train_mse'].item():.4f} | RMSE: {metrics['train_rmse'].item():.4f} | MAE: {metrics['train_mae'].item():.4f} | R²: {metrics['train_r2'].item():.4f}\")\n        \n        for var_name in self.variable_names:\n            print(f\"{var_name}:\")\n            print(f\"  MSE: {metrics[f'train_{var_name}_mse'].item():.4f} | RMSE: {metrics[f'train_{var_name}_rmse'].item():.4f}\")\n            print(f\"  MAE: {metrics[f'train_{var_name}_mae'].item():.4f} | R²: {metrics[f'train_{var_name}_r2'].item():.4f}\")\n            print(\"---\")\n\n    def on_validation_epoch_end(self):\n        # Stockage des métriques globales\n        metrics = self.trainer.callback_metrics\n        epoch = self.current_epoch\n        \n        if 'val_loss' in metrics:\n            self.val_metrics['loss'].append(metrics['val_loss'].item())\n            self.val_metrics['mse'].append(metrics['val_mse'].item())\n            self.val_metrics['rmse'].append(metrics['val_rmse'].item())\n            self.val_metrics['mae'].append(metrics['val_mae'].item())\n            self.val_metrics['r2'].append(metrics['val_r2'].item())\n        \n        # Stockage des métriques par variable\n        for var_name in self.variable_names:\n            self.val_var_metrics[var_name]['mse'].append(metrics[f'val_{var_name}_mse'].item())\n            self.val_var_metrics[var_name]['rmse'].append(metrics[f'val_{var_name}_rmse'].item())\n            self.val_var_metrics[var_name]['mae'].append(metrics[f'val_{var_name}_mae'].item())\n            self.val_var_metrics[var_name]['r2'].append(metrics[f'val_{var_name}_r2'].item())\n        \n        # Affichage comme dans la capture d'écran\n        print(f\"\\nEpoch {epoch} - Validation Metrics by Variable:\")\n        for var_name in self.variable_names:\n            print(f\"{var_name}:\")\n            print(f\"    MSE: {metrics[f'val_{var_name}_mse'].item():.4f}, RMSE: {metrics[f'val_{var_name}_rmse'].item():.4f}\")\n            print(f\"    MAE: {metrics[f'val_{var_name}_mae'].item():.4f}, R²: {metrics[f'val_{var_name}_r2'].item():.4f}\")\n            print(\"---\")\n\n    def on_test_epoch_end(self):\n        metrics = self.trainer.callback_metrics\n        \n        # Affichage des résultats finaux comme dans la capture d'écran\n        print(\"\\nFinal Test Metrics by Variable:\")\n        for var_name in self.variable_names:\n            print(f\"{var_name}:\")\n            print(f\"    MSE: {metrics[f'test_{var_name}_mse'].item():.4f}, RMSE: {metrics[f'test_{var_name}_rmse'].item():.4f}\")\n            print(f\"    MAE: {metrics[f'test_{var_name}_mae'].item():.4f}, R²: {metrics[f'test_{var_name}_r2'].item():.4f}\")\n            print(\"---\")\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=4e-4, weight_decay=1e-6)\n\n    def plot_metrics(self):\n        \"\"\"Trace les courbes d'apprentissage\"\"\"\n        plt.figure(figsize=(15, 10))\n        \n        # Plot Loss\n        plt.subplot(2, 2, 1)\n        plt.plot(self.train_metrics['loss'], 'b-', label=\"Train Loss\")\n        plt.plot(self.val_metrics['loss'], 'r-', label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Training and Validation Loss\")\n        plt.legend()\n        plt.grid()\n\n        # Plot R2\n        plt.subplot(2, 2, 2)\n        plt.plot(self.train_metrics['r2'], 'b-', label=\"Train R²\")\n        plt.plot(self.val_metrics['r2'], 'r-', label=\"Val R²\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"R² Score\")\n        plt.title(\"R² Score\")\n        plt.legend()\n        plt.grid()\n\n        # Plot RMSE\n        plt.subplot(2, 2, 3)\n        plt.plot(self.train_metrics['rmse'], 'b-', label=\"Train RMSE\")\n        plt.plot(self.val_metrics['rmse'], 'r-', label=\"Val RMSE\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"RMSE\")\n        plt.title(\"RMSE\")\n        plt.legend()\n        plt.grid()\n\n        # Plot MAE\n        plt.subplot(2, 2, 4)\n        plt.plot(self.train_metrics['mae'], 'b-', label=\"Train MAE\")\n        plt.plot(self.val_metrics['mae'], 'r-', label=\"Val MAE\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"MAE\")\n        plt.title(\"MAE\")\n        plt.legend()\n        plt.grid()\n\n        plt.tight_layout()\n        plt.savefig(\"training_metrics.png\")\n        plt.show()\n\n    def plot_variable_metrics(self):\n        \"\"\"Trace les métriques pour chaque variable séparément\"\"\"\n        fig, axs = plt.subplots(4, 4, figsize=(20, 15))\n        \n        for i, var_name in enumerate(self.variable_names):\n            # MSE\n            axs[0, i].plot(self.train_var_metrics[var_name]['mse'], 'b-', label=\"Train\")\n            axs[0, i].plot(self.val_var_metrics[var_name]['mse'], 'r-', label=\"Val\")\n            axs[0, i].set_title(f\"{var_name} - MSE\")\n            axs[0, i].grid()\n            \n            # RMSE\n            axs[1, i].plot(self.train_var_metrics[var_name]['rmse'], 'b-', label=\"Train\")\n            axs[1, i].plot(self.val_var_metrics[var_name]['rmse'], 'r-', label=\"Val\")\n            axs[1, i].set_title(f\"{var_name} - RMSE\")\n            axs[1, i].grid()\n            \n            # MAE\n            axs[2, i].plot(self.train_var_metrics[var_name]['mae'], 'b-', label=\"Train\")\n            axs[2, i].plot(self.val_var_metrics[var_name]['mae'], 'r-', label=\"Val\")\n            axs[2, i].set_title(f\"{var_name} - MAE\")\n            axs[2, i].grid()\n            \n            # R2\n            axs[3, i].plot(self.train_var_metrics[var_name]['r2'], 'b-', label=\"Train\")\n            axs[3, i].plot(self.val_var_metrics[var_name]['r2'], 'r-', label=\"Val\")\n            axs[3, i].set_title(f\"{var_name} - R²\")\n            axs[3, i].grid()\n        \n        plt.tight_layout()\n        plt.savefig(\"variable_metrics.png\")\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel5 = ASTGCNModel(\n    num_nodes=421,\n    num_features=11,\n    num_timesteps=12,\n    edge_index=edge_index31,  # Assurez-vous que edge_index1 est correctement défini\n    dropout=0.4\n)\n\n# Définir l'appareil (GPU ou CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Utilisation de {device}\")\n\n# Déplacer le modèle sur l'appareil\nmodel5.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n# Add this before initializing your model and trainer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pytorch_lightning as pl\ntrainer = pl.Trainer(max_epochs=20)\ntrainer.fit(model5, train_loader, val_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test final\nresults = trainer.test(model5, test_loader)\nprint(results)","metadata":{"id":"9TnUO2HhYMja","colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"status":"error","timestamp":1741894475925,"user_tz":-60,"elapsed":100,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"}},"outputId":"1ea7c177-99a4-44cf-97c3-e234f4dd27f1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Forme des données d'entraînement:\", train_data.shape)\nprint(\"Forme des données de validation:\", val_data.shape)\nprint(\"Forme des données de test:\", test_data.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Passer l'ensemble de test dans le modèle\nmodel5.eval()  # Mettre le modèle en mode évaluation\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel5.to(device)\n# Test loop\nwith torch.no_grad():\n    predictions= []\n    true_values = []\n    for batch in test_loader:\n        x, y = batch\n        x, y = x.to(device), y.to(device)  # Déplacer les données sur le même appareil que le modèle\n        y_hat = model5(x)\n        predictions.append(y_hat)\n        true_values.append(y)\n\n# Concaténer les résultats\npredictions = torch.cat(predictions, dim=0)\ntrue_values = torch.cat(true_values, dim=0)\n\n# Afficher les résultats\nprint(\"Predictions:\", predictions)\nprint(\"True values:\", true_values)","metadata":{"id":"8JkBrp-ooWGT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741784981206,"user_tz":-60,"elapsed":4550,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"}},"outputId":"1e5f0dbc-b63f-4d07-d1e9-47ab246f8595","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.dates as mdates\nfrom datetime import datetime, timedelta\nfrom sklearn.metrics import r2_score\n\n# Définir le jour spécifique à visualiser\nspecific_day = datetime(2021, 9, 2)  # Par exemple, le 5 mars 2025\n\n# Créer un filtre pour sélectionner uniquement les données de ce jour\n# Supposons que vous avez 12 points par jour (toutes les 2 heures)\nstart_date = datetime(2021, 6, 1)  # Date de début de vos données\ndates = [start_date + timedelta(hours=2*i) for i in range(len(true_values))]\n\n# Filtrer les indices correspondant au jour spécifique\nday_indices = [i for i, date in enumerate(dates) if date.date() == specific_day.date()]\n\n# Choisir un nœud spécifique\nnode_idx = 10\n\n# Noms des caractéristiques\nfeature_names = [\"Volume\", \"Occupation\", \"Niveau de congestion\", \"Vitesse moyenne\"]\n\n# Créer une figure 2x2 pour les 4 caractéristiques\nfig, axs = plt.subplots(2, 2, figsize=(15, 12))\naxs = axs.flatten()  # Aplatir pour un accès plus facile\n\n# Sélectionner les dates du jour spécifique\nday_dates = [dates[i] for i in day_indices]\n\n# Couleurs pour les courbes\ncolors = ['blue', 'green', 'purple', 'brown']\n\n# Tracer chaque caractéristique dans son propre sous-graphique\nfor feature_idx in range(4):\n    ax = axs[feature_idx]\n    \n    # Filtrer les données pour ce jour et cette caractéristique\n    true_day_values = true_values[day_indices, node_idx, feature_idx].cpu().numpy()\n    pred_day_values = predictions[day_indices, node_idx, feature_idx].cpu().numpy()\n    \n    # Tracer les valeurs réelles et prédites\n    ax.plot(day_dates, true_day_values, label=\"Valeurs réelles\", \n            linewidth=2, color=colors[feature_idx])\n    ax.plot(day_dates, pred_day_values, label=\"Prédictions\", \n            linewidth=2, color=colors[feature_idx], linestyle='--')\n    \n\n    \n    # Configurer l'axe des x pour afficher les heures\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n    ax.xaxis.set_major_locator(mdates.HourLocator(interval=2))  # Tick toutes les 2 heures\n    \n    # Ajouter une grille\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n  \n    \n    # Ajouter légende et titres\n    ax.legend(loc='upper right')\n    ax.set_xlabel(\"Heure\", fontsize=10)\n    ax.set_ylabel(feature_names[feature_idx], fontsize=10)\n    ax.set_title(f\"{feature_names[feature_idx]} - Capteur {node_idx}\", fontsize=12)\n\n# Titre global\nplt.suptitle(f\"Prédictions vs Valeurs réelles - {specific_day.strftime('%d/%m/%Y')}\", \n             fontsize=16, fontweight='bold', y=0.98)\n\n# Ajuster l'espacement entre les sous-graphiques\nplt.tight_layout(rect=[0, 0, 1, 0.96])  # Laisser de l'espace pour le titre global\n\n# Enregistrer le graphique\nplt.savefig(f\"prediction_sensor_{node_idx}_day_{specific_day.strftime('%Y%m%d')}.png\", \n            dpi=300, bbox_inches='tight')\n\n# Afficher le graphique\nplt.show()","metadata":{"id":"m5nwUbl1YMmI","colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"status":"ok","timestamp":1741785082100,"user_tz":-60,"elapsed":492,"user":{"displayName":"Bouaziz Omar","userId":"04710185934847346829"}},"outputId":"5ff64b8e-885c-4668-8f5f-fd9aeae9ba25","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pytorch_lightning as pl\nfrom sklearn.metrics import r2_score\nfrom torch_geometric_temporal.nn.recurrent import DCRNN\nimport matplotlib.pyplot as plt\n\nclass DCRNN_Model(pl.LightningModule):\n    def __init__(self, num_nodes, num_features, num_timesteps, adjacency_matrix, edge_index, edge_weight=None, hidden_dim=128, dropout=0.3):\n        super(DCRNN_Model, self).__init__()\n\n        self.device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        if edge_weight is None:\n            edge_weight = torch.ones(edge_index.size(1))\n        else:\n            edge_weight = torch.tensor(edge_weight, dtype=torch.float32)\n\n        self.adjacency_matrix = torch.tensor(adjacency_matrix, dtype=torch.float32, requires_grad=False).to(self.device_type)\n        self.edge_index = edge_index.to(self.device_type)\n        self.edge_weight = edge_weight.to(self.device_type)\n\n        self.num_nodes = num_nodes\n        self.num_features = num_features\n        self.num_timesteps = num_timesteps\n        self.hidden_dim = hidden_dim\n\n        self.dcrnn = DCRNN(\n            in_channels=num_features,\n            out_channels=hidden_dim,\n            K=2\n        )\n\n        self.output_layer = nn.Linear(hidden_dim, 4)\n        nn.init.kaiming_normal_(self.output_layer.weight)\n\n        self.dropout = nn.Dropout(dropout)\n\n        self.train_losses = []\n        self.val_losses = []\n        self.train_r2 = []\n        self.val_r2 = []\n\n    def forward(self, x):\n        batch_size, num_nodes, num_features, num_timesteps = x.shape\n        H = torch.zeros(batch_size, num_nodes, self.hidden_dim).to(self.device_type)\n\n        for t in range(num_timesteps):\n            X_t = x[:, :, :, t]\n            H_new = torch.zeros_like(H)\n            for b in range(batch_size):\n                H_new[b] = self.dcrnn(X_t[b], self.edge_index, self.edge_weight, H[b].detach())\n            H = H_new\n\n        H = self.dropout(H)\n        output = self.output_layer(H)\n        return output\n\n    def calculate_r2(self, y_pred, y_true):\n        y_pred_np = y_pred.detach().cpu().numpy().flatten()\n        y_true_np = y_true.detach().cpu().numpy().flatten()\n        return r2_score(y_true_np, y_pred_np)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = nn.MSELoss()(y_hat, y)\n        r2 = self.calculate_r2(y_hat, y)\n\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"train_r2\", r2, on_step=False, on_epoch=True, prog_bar=True)\n\n        if batch_idx == 0:\n            self.train_losses.append((self.current_epoch, loss.item()))\n            self.train_r2.append((self.current_epoch, r2))\n\n        return {\"loss\": loss, \"r2\": r2}\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        val_loss = nn.MSELoss()(y_hat, y)\n        val_r2 = self.calculate_r2(y_hat, y)\n\n        self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val_r2\", val_r2, on_step=False, on_epoch=True, prog_bar=True)\n\n        if batch_idx == 0:\n            self.val_losses.append((self.current_epoch, val_loss.item()))\n            self.val_r2.append((self.current_epoch, val_r2))\n\n        return {\"val_loss\": val_loss, \"val_r2\": val_r2}\n\n\n    def on_train_epoch_end(self):\n            \n        train_loss = self.trainer.callback_metrics.get(\"train_loss\")\n        train_r2 = self.trainer.callback_metrics.get(\"train_r2\")\n        \n        if train_loss is not None:\n            # Stocker l'époque avec la valeur\n            self.train_losses.append((self.current_epoch, train_loss.item()))\n            print(f\"Epoch {self.current_epoch}: Train Loss = {train_loss.item()}\")\n        \n        if train_r2 is not None:\n            # Stocker l'époque avec la valeur\n            self.train_r2.append((self.current_epoch, train_r2.item()))\n            print(f\"Epoch {self.current_epoch}: Train R² = {train_r2.item()}\")\n    \n    def on_validation_epoch_end(self):\n        val_loss = self.trainer.callback_metrics.get(\"val_loss\")\n        val_r2 = self.trainer.callback_metrics.get(\"val_r2\")\n        \n        if val_loss is not None:\n            # Stocker l'époque avec la valeur\n            self.val_losses.append((self.current_epoch, val_loss.item()))\n            print(f\"Epoch {self.current_epoch}: Val Loss = {val_loss.item()}\")\n        \n        if val_r2 is not None:\n            # Stocker l'époque avec la valeur\n            self.val_r2.append((self.current_epoch, val_r2.item()))\n            print(f\"Epoch {self.current_epoch}: Val R² = {val_r2.item()}\")\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=1e-4)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n            }\n        }\n\n    def plot_losses(self):\n        plt.figure(figsize=(12, 6))\n\n        plt.subplot(1, 2, 1)\n        if self.train_losses:\n            train_epochs = [t[0] for t in self.train_losses]\n            train_loss_values = [t[1] for t in self.train_losses]\n            plt.plot(train_epochs, train_loss_values, 'o-', label=\"Train Loss\")\n\n        if self.val_losses:\n            val_epochs = [t[0] for t in self.val_losses]\n            val_loss_values = [t[1] for t in self.val_losses]\n            plt.plot(val_epochs, val_loss_values, 'o-', label=\"Validation Loss\")\n\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss (MSE)\")\n        plt.title(\"Loss Curves\")\n        plt.legend()\n        plt.grid()\n\n        plt.subplot(1, 2, 2)\n        if self.train_r2:\n            train_r2_epochs = [t[0] for t in self.train_r2]\n            train_r2_values = [t[1] for t in self.train_r2]\n            plt.plot(train_r2_epochs, train_r2_values, 'o-', label=\"Train R²\")\n\n        if self.val_r2:\n            val_r2_epochs = [t[0] for t in self.val_r2]\n            val_r2_values = [t[1] for t in self.val_r2]\n            plt.plot(val_r2_epochs, val_r2_values, 'o-', label=\"Validation R²\")\n\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"R² Score\")\n        plt.title(\"R² Curves\")\n        plt.legend()\n        plt.grid()\n\n        plt.tight_layout()\n        plt.savefig(\"training_curves.png\")\n        plt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n# from torch_geometric_temporal.nn.recurrent import DCRNN\n\nnum_sensors = data_restructured_final.shape[1]\nnum_features = data_restructured_final.shape[2]\nnum_timesteps = 12\nmodel1 = DCRNN_Model(num_sensors, num_features, num_timesteps, adj_matrix2, edge_index=edge_index2, dropout=0.3)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Entraînement\nimport pytorch_lightning as pl\ntrainer = pl.Trainer(max_epochs=20)\ntrainer.fit(model1, train_loader, val_loader, ckpt_path= None)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test final\nresults = trainer.test(model1, test_loader)\nprint(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model1.plot_losses()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.dates as mdates\nfrom datetime import datetime, timedelta\nfrom sklearn.metrics import r2_score\n\n# Définir le jour spécifique à visualiser\nspecific_day = datetime(2021, 8, 3)  # Par exemple, le 5 mars 2025\n\n# Créer un filtre pour sélectionner uniquement les données de ce jour\n# Supposons que vous avez 12 points par jour (toutes les 2 heures)\nstart_date = datetime(2021, 6, 1)  # Date de début de vos données\ndates = [start_date + timedelta(hours=2*i) for i in range(len(true_values))]\n\n# Filtrer les indices correspondant au jour spécifique\nday_indices = [i for i, date in enumerate(dates) if date.date() == specific_day.date()]\n\n# Choisir un nœud spécifique\nnode_idx = 10\n\n# Noms des caractéristiques\nfeature_names = [\"Volume\", \"Occupation\", \"Niveau de congestion\", \"Vitesse moyenne\"]\n\n# Créer une figure 2x2 pour les 4 caractéristiques\nfig, axs = plt.subplots(2, 2, figsize=(15, 12))\naxs = axs.flatten()  # Aplatir pour un accès plus facile\n\n# Sélectionner les dates du jour spécifique\nday_dates = [dates[i] for i in day_indices]\n\n# Couleurs pour les courbes\ncolors = ['blue', 'green', 'purple', 'brown']\n\n# Tracer chaque caractéristique dans son propre sous-graphique\nfor feature_idx in range(4):\n    ax = axs[feature_idx]\n    \n    # Filtrer les données pour ce jour et cette caractéristique\n    true_day_values = true_values[day_indices, node_idx, feature_idx].cpu().numpy()\n    pred_day_values = predictions[day_indices, node_idx, feature_idx].cpu().numpy()\n    \n    # Tracer les valeurs réelles et prédites\n    ax.plot(day_dates, true_day_values, label=\"Valeurs réelles\", \n            linewidth=2, color=colors[feature_idx])\n    ax.plot(day_dates, pred_day_values, label=\"Prédictions\", \n            linewidth=2, color=colors[feature_idx], linestyle='--')\n    \n\n    \n    # Configurer l'axe des x pour afficher les heures\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n    ax.xaxis.set_major_locator(mdates.HourLocator(interval=2))  # Tick toutes les 2 heures\n    \n    # Ajouter une grille\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n  \n    \n    # Ajouter légende et titres\n    ax.legend(loc='upper right')\n    ax.set_xlabel(\"Heure\", fontsize=10)\n    ax.set_ylabel(feature_names[feature_idx], fontsize=10)\n    ax.set_title(f\"{feature_names[feature_idx]} - Capteur {node_idx}\", fontsize=12)\n\n# Titre global\nplt.suptitle(f\"Prédictions vs Valeurs réelles - {specific_day.strftime('%d/%m/%Y')}\", \n             fontsize=16, fontweight='bold', y=0.98)\n\n# Ajuster l'espacement entre les sous-graphiques\nplt.tight_layout(rect=[0, 0, 1, 0.96])  # Laisser de l'espace pour le titre global\n\n# Enregistrer le graphique\nplt.savefig(f\"prediction_sensor_{node_idx}_day_{specific_day.strftime('%Y%m%d')}.png\", \n            dpi=300, bbox_inches='tight')\n\n# Afficher le graphique\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Passer l'ensemble de test dans le modèle\nmodel1.eval()  # Mettre le modèle en mode évaluation\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel1.to(device)\n# Test loop\nwith torch.no_grad():\n    predictions = []\n    true_values = []\n    for batch in test_loader:\n        x, y = batch\n        x, y = x.to(device), y.to(device)  # Déplacer les données sur le même appareil que le modèle\n        y_hat = model1(x)\n        predictions.append(y_hat)\n        true_values.append(y)\n\n# Concaténer les résultats\npredictions = torch.cat(predictions, dim=0)\ntrue_values = torch.cat(true_values, dim=0)\n\n# Afficher les résultats\nprint(\"Predictions:\", predictions)\nprint(\"True values:\", true_values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Afficher les prédictions et les vraies valeurs pour un nœud donné\nnode_idx = 50 # Choisir un nœud spécifique\nplt.figure(figsize=(12, 6))  # Taille de la figure\nplt.plot(true_values[:, node_idx, 0].cpu().numpy(), label=\"True values\")\nplt.plot(predictions[:, node_idx, 0].cpu().numpy(), label=\"Predictions\")\n\n# Ajuster l'échelle de l'axe des temps\nnum_time_steps = len(true_values[:, node_idx, 0])  # Nombre total de time steps\nstep = 200  # Afficher une valeur tous les 100 time steps\nplt.xticks(range(0, num_time_steps, step), rotation=45)  # Rotation pour une meilleure lisibilité\n\n# Ajouter une grille\nplt.grid(True, linestyle='--', alpha=0.6)\n\nplt.legend()\nplt.xlabel(\"Time steps\")\nplt.ylabel(\"Value\")\nplt.title(\"Predictions vs True values\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --upgrade optuna pytorch-lightning optuna-integration","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model2.plot_losses()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test final\nresults = trainer.test(model2, test_loader)\nprint(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install optuna ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\nimport joblib\nimport torch\nimport pytorch_lightning as pl\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import r2_score\nimport matplotlib.pyplot as plt\n\nclass OptimizedASTGCNModel(pl.LightningModule):\n    def __init__(self, num_nodes, edge_index, num_features=11, num_timesteps=12, \n                 output_size=4, trial=None):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Paramètres optimisés\n        self.params = self.get_optimized_params(trial)\n        \n        self.astgcn = ASTGCN(\n            nb_block=self.params['nb_block'],\n            in_channels=num_features,\n            K=self.params['K'],\n            nb_chev_filter=self.params['nb_chev_filter'],\n            nb_time_filter=self.params['nb_time_filter'],\n            time_strides=1,\n            num_for_predict=num_timesteps,\n            len_input=num_timesteps,\n            num_of_vertices=num_nodes,\n        )\n        \n        # Architecture dynamique\n        self.adapter = self.create_adapter_layer()\n        self.output_layer = nn.Linear(self.params['adapter_output_dim'], num_nodes * output_size)\n        self.dropout = nn.Dropout(self.params['dropout'])\n        \n        # Tracking des métriques\n        self.metrics = {\n            'train': {'loss': [], 'r2': []},\n            'val': {'loss': [], 'r2': []},\n            'test': {'loss': None, 'r2': None}\n        }\n\n    def get_optimized_params(self, trial):\n        if trial:\n            return {\n                'nb_block': trial.suggest_int('nb_block', 1, 3),\n                'K': trial.suggest_int('K', 2, 5),\n                'nb_chev_filter': trial.suggest_categorical('nb_chev_filter', [32, 64, 128]),\n                'nb_time_filter': trial.suggest_categorical('nb_time_filter', [32, 64, 128]),\n                'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n                'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n                'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),\n                'use_layer_norm': trial.suggest_categorical('use_layer_norm', [True, False]),\n                'adapter_hidden_dim': trial.suggest_categorical('adapter_hidden_dim', [256, 512, 1024]),\n                'adapter_output_dim': None  # Será calculé dans create_adapter_layer\n            }\n        else:\n            # Valeurs par défaut\n            return {\n                'nb_block': 2,\n                'K': 3,\n                'nb_chev_filter': 64,\n                'nb_time_filter': 64,\n                'dropout': 0.2,\n                'learning_rate': 1e-3,\n                'weight_decay': 1e-5,\n                'use_layer_norm': True,\n                'adapter_hidden_dim': 512,\n                'adapter_output_dim': 512\n            }\n\n    def create_adapter_layer(self):\n        layers = []\n        input_dim = self.hparams.num_nodes * self.params['nb_time_filter']\n        \n        if self.params['adapter_hidden_dim'] > 0:\n            layers.append(nn.Linear(input_dim, self.params['adapter_hidden_dim']))\n            if self.params['use_layer_norm']:\n                layers.append(nn.LayerNorm(self.params['adapter_hidden_dim']))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(self.params['dropout']/2))\n            self.params['adapter_output_dim'] = self.params['adapter_hidden_dim']\n        else:\n            self.params['adapter_output_dim'] = input_dim\n        \n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = x.to(self.device)\n        self.hparams.edge_index = self.hparams.edge_index.to(self.device)\n        \n        x = self.astgcn(x, self.hparams.edge_index)\n        x = x.reshape(x.size(0), -1)\n        x = self.adapter(x)\n        x = self.output_layer(x)\n        x = self.dropout(x)\n        return x.reshape(x.size(0), self.hparams.num_nodes, self.hparams.output_size)\n\n    def configure_optimizers(self):\n        optimizer = optim.AdamW(\n            self.parameters(),\n            lr=self.params['learning_rate'],\n            weight_decay=self.params['weight_decay']\n        )\n        \n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer,\n            mode='min',\n            factor=0.5,\n            patience=5,\n            verbose=True\n        )\n        \n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'monitor': 'val_loss',\n                'interval': 'epoch',\n                'frequency': 1\n            }\n        }\n\n    def log_metrics(self, phase, loss, y, y_hat):\n        self.log(f\"{phase}_loss\", loss, prog_bar=True)\n        r2 = r2_score(y.cpu().numpy().flatten(), y_hat.cpu().detach().numpy().flatten())\n        self.log(f\"{phase}_r2\", r2, prog_bar=True)\n        \n        if phase in ['train', 'val']:\n            self.metrics[phase]['loss'].append(loss.item())\n            self.metrics[phase]['r2'].append(r2)\n        else:\n            self.metrics[phase]['loss'] = loss.item()\n            self.metrics[phase]['r2'] = r2\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = nn.MSELoss()(y_hat, y)\n        self.log_metrics('train', loss, y, y_hat)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = nn.MSELoss()(y_hat, y)\n        self.log_metrics('val', loss, y, y_hat)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = nn.MSELoss()(y_hat, y)\n        self.log_metrics('test', loss, y, y_hat)\n        return loss\n\n    def plot_metrics(self):\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n        \n        # Loss plot\n        ax1.plot(self.metrics['train']['loss'], label='Train Loss')\n        ax1.plot(self.metrics['val']['loss'], label='Validation Loss')\n        ax1.set_title('Loss Evolution')\n        ax1.set_xlabel('Epochs')\n        ax1.set_ylabel('MSE Loss')\n        ax1.legend()\n        ax1.grid()\n        \n        # R2 plot\n        ax2.plot(self.metrics['train']['r2'], label='Train R²')\n        ax2.plot(self.metrics['val']['r2'], label='Validation R²')\n        ax2.set_title('R² Score Evolution')\n        ax2.set_xlabel('Epochs')\n        ax2.set_ylabel('R² Score')\n        ax2.legend()\n        ax2.grid()\n        \n        plt.tight_layout()\n        plt.savefig('training_metrics.png')\n        plt.show()\n\ndef optimize_hyperparameters():\n    study = optuna.create_study(\n        direction='minimize',\n        sampler=optuna.samplers.TPESampler(),\n        pruner=optuna.pruners.MedianPruner()\n    )\n    \n    def objective(trial):\n        model = OptimizedASTGCNModel(\n            num_nodes=num_sensors,\n            edge_index=edge_index1,\n            num_features=num_features,\n            num_timesteps=num_timesteps,\n            output_size=4,\n            trial=trial\n        )\n        \n        trainer = pl.Trainer(\n            max_epochs=30,\n            accelerator='auto',\n            devices=1 if torch.cuda.is_available() else None,\n            callbacks=[\n                PyTorchLightningPruningCallback(trial, monitor='val_loss'),\n                pl.callbacks.EarlyStopping(monitor='val_loss', patience=7)\n            ],\n            enable_progress_bar=True,\n            enable_model_summary=True\n        )\n        \n        trainer.fit(model)\n        return trainer.callback_metrics['val_loss'].item()\n    \n    study.optimize(objective, n_trials=50, timeout=3600*3)\n    \n    # Sauvegarde et visualisation\n    joblib.dump(study, 'astgcn_study.pkl')\n    plot_optimization_history(study).show()\n    plot_param_importances(study).show()\n    \n    return study.best_params","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_final_model(best_params):\n    model = OptimizedASTGCNModel(\n        num_nodes=num_sensors,\n        edge_index=edge_index1,\n        num_features=num_features,\n        num_timesteps=num_timesteps,\n        output_size=4,\n        trial=None\n    )\n    \n    # Mise à jour des paramètres avec les meilleures valeurs\n    model.hparams.params.update(best_params)\n    \n    trainer = pl.Trainer(\n        max_epochs=20,\n        accelerator='auto',\n        devices=1 if torch.cuda.is_available() else None,\n        callbacks=[\n            pl.callbacks.ModelCheckpoint(monitor='val_loss'),\n            pl.callbacks.LearningRateMonitor()\n        ]\n    )\n    \n    trainer.fit(model)\n    test_results = trainer.test(model)\n    \n    # Sauvegarde du modèle\n    torch.save(model.state_dict(), 'best_astgcn_model.pth')\n    \n    # Visualisation\n    model.plot_metrics()\n    \n    return model, test_results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\n# Exécution complète\n\nif __name__ == '__main__':\n    # Phase d'optimisation\n    best_params = optimize_hyperparameters()\n    print(\"Meilleurs paramètres trouvés :\", best_params)\n    \n    # Phase d'entraînement final\n    final_model, test_results = train_final_model(best_params)\n    print(\"Résultats du test :\", test_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install optuna-integration[pytorch_lightning]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom optuna.integration import PyTorchLightningPruningCallback\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\nimport joblib\nimport torch\nimport pytorch_lightning as pl\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import r2_score\nimport matplotlib.pyplot as plt\nimport logging\n\n# Configuration du logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass OptimizedASTGCNModel(pl.LightningModule):\n    def __init__(self, num_nodes, edge_index, num_features=11, num_timesteps=12, \n                 output_size=4, trial=None):\n        super().__init__()\n        self.save_hyperparameters()\n        self.edge_index = edge_index\n        \n        self.params = self.get_optimized_params(trial)\n        logger.info(f\"Initialisation du modèle avec num_nodes={num_nodes}\")\n        logger.debug(f\"Paramètres optimisés: {self.params}\")\n\n        try:\n            self.astgcn = ASTGCN(\n                nb_block=self.params['nb_block'],\n                in_channels=num_features,\n                K=self.params['K'],\n                nb_chev_filter=self.params['nb_chev_filter'],\n                nb_time_filter=self.params['nb_time_filter'],\n                time_strides=1,\n                num_for_predict=num_timesteps,\n                len_input=num_timesteps,\n                num_of_vertices=num_nodes,\n            )\n\n            self.adapter = self.create_adapter_layer()\n            adapter_output_dim = self.params['adapter_output_dim']\n            self.output_layer = nn.Linear(adapter_output_dim, num_nodes * output_size)\n            self.dropout = nn.Dropout(self.params['dropout'])\n        \n        except Exception as e:\n            logger.error(f\"Erreur lors de l'initialisation du modèle: {str(e)}\")\n            raise\n\n    def create_adapter_layer(self):\n        layers = []\n        input_dim = self.hparams.num_nodes * self.params['nb_time_filter']\n\n        if input_dim <= 0:\n            raise ValueError(f\"Dimension d'entrée invalide: {input_dim}\")\n        \n        if self.params['adapter_hidden_dim'] > 0:\n            layers.append(nn.Linear(input_dim, self.params['adapter_hidden_dim']))\n            if self.params['use_layer_norm']:\n                layers.append(nn.LayerNorm(self.params['adapter_hidden_dim']))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(self.params['dropout'] / 2))\n            self.params['adapter_output_dim'] = self.params['adapter_hidden_dim']\n        else:\n            self.params['adapter_output_dim'] = input_dim\n        \n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = x.to(self.device)\n        self.edge_index = self.edge_index.to(self.device)\n        \n        x = self.astgcn(x, self.edge_index)\n        x = x.reshape(x.size(0), -1)\n        x = self.adapter(x)\n        x = self.output_layer(x)\n        x = self.dropout(x)\n        return x.reshape(x.size(0), self.hparams.num_nodes, self.hparams.output_size)\n\n    def configure_optimizers(self):\n        optimizer = optim.AdamW(\n            self.parameters(),\n            lr=self.params['learning_rate'],\n            weight_decay=self.params['weight_decay']\n        )\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n        )\n        return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'monitor': 'val_loss'}}\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = nn.MSELoss()(y_hat, y)\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = nn.MSELoss()(y_hat, y)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = nn.MSELoss()(y_hat, y)\n        self.log(\"test_loss\", loss, prog_bar=True)\n        return loss\n\ndef optimize_hyperparameters():\n    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler())\n    \n    def objective(trial):\n        model = OptimizedASTGCNModel(\n            num_nodes=num_sensors,\n            edge_index=edge_index1,\n            num_features=num_features,\n            num_timesteps=num_timesteps,\n            output_size=4,\n            trial=trial\n        )\n        \n        trainer = pl.Trainer(\n            max_epochs=30,\n            accelerator='auto',\n            devices=1 if torch.cuda.is_available() else None,\n            callbacks=[\n                PyTorchLightningPruningCallback(trial, monitor='val_loss'),\n                pl.callbacks.EarlyStopping(monitor='val_loss', patience=7)\n            ],\n            enable_progress_bar=False\n        )\n        trainer.fit(model)\n        return trainer.callback_metrics['val_loss'].item()\n    \n    study.optimize(objective, n_trials=50, timeout=10800)\n    joblib.dump(study, 'astgcn_study.pkl')\n    plot_optimization_history(study).show()\n    plot_param_importances(study).show()\n    \n    return study.best_params\n\ndef train_final_model(best_params):\n    model = OptimizedASTGCNModel(\n        num_nodes=num_sensors,\n        edge_index=edge_index1,\n        num_features=num_features,\n        num_timesteps=num_timesteps,\n        output_size=4\n    )\n    model.hparams.params.update(best_params)\n    \n    trainer = pl.Trainer(\n        max_epochs=100,\n        accelerator='auto',\n        devices=1 if torch.cuda.is_available() else None,\n        callbacks=[\n            pl.callbacks.ModelCheckpoint(monitor='val_loss'),\n            pl.callbacks.LearningRateMonitor()\n        ]\n    )\n    trainer.fit(model)\n    trainer.test(model)\n    torch.save(model.state_dict(), 'best_astgcn_model.pth')\n    \n    return model\n\nif __name__ == '__main__':\n    best_params = optimize_hyperparameters()\n    print(\"Meilleurs paramètres trouvés :\", best_params)\n    final_model = train_final_model(best_params)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install optuna-integration[pytorch_lightning]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom optuna.integration import PyTorchLightningPruningCallback\nfrom pytorch_lightning.callbacks import EarlyStopping\nimport pytorch_lightning as pl\n\ndef objective(trial):\n    # Hyperparamètres à optimiser\n    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n    nb_chev_filter = trial.suggest_categorical('nb_chev_filter', [16, 32, 64])\n    nb_time_filter = trial.suggest_categorical('nb_time_filter', [16, 32, 64])\n    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True)\n    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n\n    # Créer le modèle avec les hyperparamètres suggérés\n    model = ASTGCNModel(\n        num_nodes=421,\n        edge_index=edge_index1,\n        num_features=11,\n        num_timesteps=24,\n        output_features=4,\n        dropout=dropout,\n        nb_chev_filter=nb_chev_filter,\n        nb_time_filter=nb_time_filter\n    )\n    \n    # Mettre à jour les paramètres de l'optimiseur\n    model.configure_optimizers = lambda: optim.Adam(\n        model.parameters(), \n        lr=lr, \n        weight_decay=weight_decay\n    )\n\n    # Configuration des callbacks\n    early_stopping = EarlyStopping(\n        monitor=\"val_loss\",\n        patience=5,\n        mode=\"min\",\n        verbose=True\n    )\n    \n\n    # Configuration de l'entraîneur\n    trainer = pl.Trainer(\n        max_epochs=30,\n        callbacks=[early_stopping],\n        enable_progress_bar=True,\n        enable_model_summary=True,\n        accelerator='auto',\n        devices='auto'\n    )\n\n    # Entraîner le modèle\n    trainer.fit(model)\n\n    # Retourner la meilleure valeur de val_loss\n    return trainer.callback_metrics[\"val_loss\"].item()\n\n# Étude Optuna\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=30, timeout=3600)\n\n# Afficher les résultats\nprint(\"\\nRésultats de l'optimisation:\")\nprint(\"Nombre d'essais: \", len(study.trials))\nprint(\"Meilleur essai:\")\ntrial = study.best_trial\nprint(f\"  Valeur (val_loss): {trial.value:.4f}\")\nprint(\"  Paramètres: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\n# Entraîner le modèle final avec les meilleurs hyperparamètres\nbest_params = study.best_params\nfinal_model = ASTGCNModel(\n    num_nodes=421,\n    edge_index=edge_index1,\n    num_features=11,\n    num_timesteps=24,\n    output_features=4,\n    dropout=best_params['dropout'],\n    nb_chev_filter=best_params['nb_chev_filter'],\n    nb_time_filter=best_params['nb_time_filter']\n)\n\n# Configuration finale\nfinal_trainer = pl.Trainer(\n    max_epochs=100,\n    accelerator='auto',\n    devices=1,\n    enable_progress_bar=True,\n    enable_model_summary=True\n)\n\n# Entraînement final\nfinal_trainer.fit(final_model)\n\n# Visualisation des résultats\nfinal_model.plot_losses()\n\n# Test final\ntest_results = final_trainer.test(final_model)\nprint(\"\\nRésultats du test:\")\nprint(f\"Test Loss: {test_results[0]['test_loss']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\nos.environ['PYTHONHASHSEED'] = '42'\n\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch_geometric_temporal.nn import MTGNN\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom typing import Dict, List, Tuple\n\n# Seed everything pour reproductibilité\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.use_deterministic_algorithms(False)\n\nseed_everything()\n\n# Paramètres\nNUM_SENSORS = 421\nNUM_FEATURES = 11\nNUM_TIMESTEPS = 12\nHORIZON = 1\nBATCH_SIZE = 32\nLEARNING_RATE = 0.001\nEPOCHS = 20\n\n# -------------------------------------------------\n# 1. Préparation des données (simulées pour l'exemple)\n# -------------------------------------------------\n# Génération de données aléatoires pour l'exemple\ndata_tensor = torch.tensor(data_restructured_final, dtype=torch.float32)\n\ndef create_sequences(data, seq_len, horizon):\n    sequences = []\n    targets = []\n    for i in range(len(data) - seq_len - horizon):\n        seq = data[i:i+seq_len]  # (seq_len, num_nodes, num_features)\n        target = data[i+seq_len, :, :4]  # (num_nodes, 4)\n        sequences.append(seq)\n        targets.append(target)\n    return torch.stack(sequences), torch.stack(targets)\n\nsequences, targets = create_sequences(data_tensor, NUM_TIMESTEPS, HORIZON)\n\n# Split des données\ntrain_seq, temp_seq, train_targ, temp_targ = train_test_split(\n    sequences, targets, test_size=0.2, random_state=42\n)\nval_seq, test_seq, val_targ, test_targ = train_test_split(\n    temp_seq, temp_targ, test_size=0.5, random_state=42\n)\n\n# -------------------------------------------------\n# 2. Dataset et DataLoader\n# -------------------------------------------------\nclass TrafficDataset(Dataset):\n    def __init__(self, sequences, targets):\n        self.sequences = sequences\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        return self.sequences[idx], self.targets[idx]\n\n# Création des datasets\ntrain_dataset = TrafficDataset(train_seq, train_targ)\nval_dataset = TrafficDataset(val_seq, val_targ)\ntest_dataset = TrafficDataset(test_seq, test_targ)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, persistent_workers=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4, persistent_workers=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4, persistent_workers=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pytorch_lightning as pl\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport networkx as nx\nimport torch.nn.functional as F\n\nclass TrafficModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.variable_names = ['volume', 'occupation', 'congestion_level', 'speed_avg']\n        \n        self.feature_transform = nn.Sequential(\n            nn.Conv2d(\n                in_channels=NUM_FEATURES, \n                out_channels=32,\n                kernel_size=1\n            ),\n            nn.ReLU()\n        )\n        \n        self.model = MTGNN(\n            gcn_true=True,\n            build_adj=True,\n            gcn_depth=2,\n            num_nodes=NUM_SENSORS,\n            kernel_size=2,\n            kernel_set=[1, 2],\n            subgraph_size=20,\n            node_dim=40,\n            dilation_exponential=1,\n            conv_channels=32,\n            residual_channels=32,\n            skip_channels=64,\n            end_channels=128,\n            seq_length=NUM_TIMESTEPS,\n            in_dim=32,\n            out_dim=4,\n            layers=2,\n            propalpha=0.05,\n            tanhalpha=3,\n            layer_norm_affline=True,\n            dropout=0.2\n        )\n        \n        self.loss = nn.MSELoss()\n        self.save_hyperparameters()\n        \n        # Métriques globales\n        self.train_metrics = {'epoch': [], 'loss': [], 'mse': [], 'rmse': [], 'mae': [], 'r2': []}\n        self.val_metrics = {'epoch': [], 'loss': [], 'mse': [], 'rmse': [], 'mae': [], 'r2': []}\n        self.test_metrics = {'epoch': [], 'loss': [], 'mse': [], 'rmse': [], 'mae': [], 'r2': []}\n        \n        # Métriques par variable\n        self.train_var_metrics = {var: {'mse': [], 'rmse': [], 'mae': [], 'r2': []} for var in self.variable_names}\n        self.val_var_metrics = {var: {'mse': [], 'rmse': [], 'mae': [], 'r2': []} for var in self.variable_names}\n        self.test_var_metrics = {var: {'mse': [], 'rmse': [], 'mae': [], 'r2': []} for var in self.variable_names}\n\n    def forward(self, x):\n        x = x.permute(0, 3, 2, 1)\n        x = self.feature_transform(x)\n        output = self.model(x)\n        return output.squeeze(-1).permute(0, 2, 1)\n\n    def _compute_metrics(self, y_hat, y):\n        y_hat_np = y_hat.detach().cpu().numpy().flatten()\n        y_np = y.detach().cpu().numpy().flatten()\n        \n        return {\n            'mse': mean_squared_error(y_np, y_hat_np),\n            'rmse': np.sqrt(mean_squared_error(y_np, y_hat_np)),\n            'mae': mean_absolute_error(y_np, y_hat_np),\n            'r2': r2_score(y_np, y_hat_np)\n        }\n\n    def _compute_metrics_by_variable(self, y_hat, y):\n        metrics = {}\n        for i, var_name in enumerate(self.variable_names):\n            y_hat_var = y_hat[..., i].flatten().detach().cpu().numpy()\n            y_var = y[..., i].flatten().detach().cpu().numpy()\n            \n            metrics[var_name] = {\n                'mse': mean_squared_error(y_var, y_hat_var),\n                'rmse': np.sqrt(mean_squared_error(y_var, y_hat_var)),\n                'mae': mean_absolute_error(y_var, y_hat_var),\n                'r2': r2_score(y_var, y_hat_var)\n            }\n        return metrics\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.loss(y_hat, y)\n        \n        # Calcul des métriques\n        metrics = self._compute_metrics(y_hat, y)\n        var_metrics = self._compute_metrics_by_variable(y_hat, y)\n        \n        # Logging global\n        self.log('train_loss', loss, prog_bar=True)\n        self.log('train_mse', metrics['mse'], prog_bar=False)\n        self.log('train_rmse', metrics['rmse'], prog_bar=True)\n        self.log('train_mae', metrics['mae'], prog_bar=False)\n        self.log('train_r2', metrics['r2'], prog_bar=True)\n        \n        # Logging par variable\n        for var_name in self.variable_names:\n            self.log(f'train_{var_name}_mse', var_metrics[var_name]['mse'], prog_bar=False)\n            self.log(f'train_{var_name}_rmse', var_metrics[var_name]['rmse'], prog_bar=False)\n            self.log(f'train_{var_name}_mae', var_metrics[var_name]['mae'], prog_bar=False)\n            self.log(f'train_{var_name}_r2', var_metrics[var_name]['r2'], prog_bar=False)\n        \n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.loss(y_hat, y)\n        \n        # Calcul des métriques\n        metrics = self._compute_metrics(y_hat, y)\n        var_metrics = self._compute_metrics_by_variable(y_hat, y)\n        \n        # Logging global\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_mse', metrics['mse'], prog_bar=False)\n        self.log('val_rmse', metrics['rmse'], prog_bar=True)\n        self.log('val_mae', metrics['mae'], prog_bar=False)\n        self.log('val_r2', metrics['r2'], prog_bar=True)\n        \n        # Logging par variable\n        for var_name in self.variable_names:\n            self.log(f'val_{var_name}_mse', var_metrics[var_name]['mse'], prog_bar=False)\n            self.log(f'val_{var_name}_rmse', var_metrics[var_name]['rmse'], prog_bar=False)\n            self.log(f'val_{var_name}_mae', var_metrics[var_name]['mae'], prog_bar=False)\n            self.log(f'val_{var_name}_r2', var_metrics[var_name]['r2'], prog_bar=False)\n        \n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.loss(y_hat, y)\n        \n        # Calcul des métriques\n        metrics = self._compute_metrics(y_hat, y)\n        var_metrics = self._compute_metrics_by_variable(y_hat, y)\n        \n        # Logging global\n        self.log('test_loss', loss)\n        self.log('test_mse', metrics['mse'])\n        self.log('test_rmse', metrics['rmse'])\n        self.log('test_mae', metrics['mae'])\n        self.log('test_r2', metrics['r2'])\n        \n        # Logging par variable\n        for var_name in self.variable_names:\n            self.log(f'test_{var_name}_mse', var_metrics[var_name]['mse'])\n            self.log(f'test_{var_name}_rmse', var_metrics[var_name]['rmse'])\n            self.log(f'test_{var_name}_mae', var_metrics[var_name]['mae'])\n            self.log(f'test_{var_name}_r2', var_metrics[var_name]['r2'])\n        \n        return {\n            'test_loss': loss,\n            'metrics': metrics,\n            'var_metrics': var_metrics\n        }\n\n    def on_train_epoch_end(self):\n        metrics = self.trainer.callback_metrics\n        epoch = self.current_epoch\n        \n        # Stockage métriques globales\n        self.train_metrics['epoch'].append(epoch)\n        for key in ['loss', 'mse', 'rmse', 'mae', 'r2']:\n            self.train_metrics[key].append(metrics[f'train_{key}'].item())\n        \n        # Stockage métriques par variable\n        for var_name in self.variable_names:\n            for metric in ['mse', 'rmse', 'mae', 'r2']:\n                self.train_var_metrics[var_name][metric].append(metrics[f'train_{var_name}_{metric}'].item())\n        \n        # Affichage\n        print(f\"\\nEpoch {epoch} - Train Metrics:\")\n        print(f\"Global - Loss: {metrics['train_loss'].item():.4f} | MSE: {metrics['train_mse'].item():.4f} | RMSE: {metrics['train_rmse'].item():.4f} | MAE: {metrics['train_mae'].item():.4f} | R²: {metrics['train_r2'].item():.4f}\")\n        \n        for var_name in self.variable_names:\n            print(f\"{var_name}:\")\n            print(f\"    MSE: {metrics[f'train_{var_name}_mse'].item():.4f}, RMSE: {metrics[f'train_{var_name}_rmse'].item():.4f}\")\n            print(f\"    MAE: {metrics[f'train_{var_name}_mae'].item():.4f}, R²: {metrics[f'train_{var_name}_r2'].item():.4f}\")\n            print(\"---\")\n\n    def on_validation_epoch_end(self):\n        metrics = self.trainer.callback_metrics\n        epoch = self.current_epoch\n        \n        # Stockage métriques globales\n        self.val_metrics['epoch'].append(epoch)\n        for key in ['loss', 'mse', 'rmse', 'mae', 'r2']:\n            self.val_metrics[key].append(metrics[f'val_{key}'].item())\n        \n        # Stockage métriques par variable\n        for var_name in self.variable_names:\n            for metric in ['mse', 'rmse', 'mae', 'r2']:\n                self.val_var_metrics[var_name][metric].append(metrics[f'val_{var_name}_{metric}'].item())\n        \n        # Affichage formaté comme dans la capture d'écran\n        print(f\"\\nEpoch {epoch} - Validation Metrics by Variable:\")\n        for var_name in self.variable_names:\n            print(f\"{var_name}:\")\n            print(f\"    MSE: {metrics[f'val_{var_name}_mse'].item():.4f}, RMSE: {metrics[f'val_{var_name}_rmse'].item():.4f}\")\n            print(f\"    MAE: {metrics[f'val_{var_name}_mae'].item():.4f}, R²: {metrics[f'val_{var_name}_r2'].item():.4f}\")\n            print(\"---\")\n\n    def on_test_epoch_end(self):\n        metrics = self.trainer.callback_metrics\n        \n        # Stockage métriques globales\n        self.test_metrics['epoch'].append(0)\n        for key in ['loss', 'mse', 'rmse', 'mae', 'r2']:\n            self.test_metrics[key].append(metrics[f'test_{key}'].item())\n        \n        # Stockage métriques par variable\n        for var_name in self.variable_names:\n            for metric in ['mse', 'rmse', 'mae', 'r2']:\n                self.test_var_metrics[var_name][metric].append(metrics[f'test_{var_name}_{metric}'].item())\n        \n        # Affichage final comme dans la capture d'écran\n        print(\"\\nFinal Test Metrics by Variable:\")\n        for var_name in self.variable_names:\n            print(f\"{var_name}:\")\n            print(f\"    MSE: {metrics[f'test_{var_name}_mse'].item():.4f}, RMSE: {metrics[f'test_{var_name}_rmse'].item():.4f}\")\n            print(f\"    MAE: {metrics[f'test_{var_name}_mae'].item():.4f}, R²: {metrics[f'test_{var_name}_r2'].item():.4f}\")\n            print(\"---\")\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, \n            mode='min',\n            factor=0.5,\n            patience=3,\n            verbose=True\n        )\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n            }\n        }\n\n    def get_adjacency_matrix(self):\n        \"\"\"Retourne la matrice d'adjacence apprise par le modèle\"\"\"\n        graph_constructor = self.model._graph_constructor\n        \n        if hasattr(graph_constructor, 'adj'):\n            return graph_constructor.adj.detach().cpu().numpy()\n        \n        if hasattr(graph_constructor, 'build_adjacency'):\n            with torch.no_grad():\n                return graph_constructor.build_adjacency().detach().cpu().numpy()\n        \n        if hasattr(graph_constructor, '_embedding1') and hasattr(graph_constructor, '_embedding2'):\n            with torch.no_grad():\n                nodevec1 = graph_constructor._embedding1.weight\n                nodevec2 = graph_constructor._embedding2.weight\n                adj = torch.mm(nodevec1, nodevec2.transpose(1, 0))\n                adj = F.softmax(F.relu(adj), dim=1)\n                return adj.detach().cpu().numpy()\n        \n        print(\"Structure du graph constructor:\")\n        for name, param in graph_constructor.named_parameters():\n            print(f\"{name}: {param.shape}\")\n        \n        raise AttributeError(\"Impossible de trouver la matrice d'adjacence dans le graph constructor\")\n\n    def plot_weighted_graph(self, top_k=20, figsize=(15, 12)):\n        adj_matrix = self.get_adjacency_matrix()\n        sorted_indices = np.dstack(np.unravel_index(np.argsort(-adj_matrix.ravel()), adj_matrix.shape))[0]\n        \n        G = nx.Graph()\n        for i in range(min(top_k, len(sorted_indices))):\n            u, v = sorted_indices[i]\n            weight = adj_matrix[u, v]\n            if weight > 0:\n                G.add_edge(u, v, weight=weight)\n        \n        pos = nx.spring_layout(G, k=0.5)\n        plt.figure(figsize=figsize)\n        \n        nx.draw_networkx_nodes(G, pos, node_size=800, node_color='lightcoral', alpha=0.9)\n        \n        edges = G.edges(data=True)\n        widths = [2 + 5 * d['weight'] for _, _, d in edges]\n        nx.draw_networkx_edges(G, pos, width=widths, alpha=0.6, edge_color='dimgray')\n        \n        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n        \n        edge_labels = {(u, v): f\"{d['weight']:.2f}\" for u, v, d in edges}\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n        \n        plt.title(f\"Top {top_k} connexions les plus fortes\")\n        plt.axis('off')\n        plt.tight_layout()\n        plt.savefig('weighted_graph.png', dpi=300)\n        plt.show()\n    \n    def plot_metrics(self):\n        \"\"\"Trace les courbes d'apprentissage globales\"\"\"\n        try:\n            plt.figure(figsize=(18, 12))\n            \n            # Loss\n            plt.subplot(2, 2, 1)\n            plt.plot(self.train_metrics['epoch'], self.train_metrics['loss'], 'b-', label='Train')\n            plt.plot(self.val_metrics['epoch'], self.val_metrics['loss'], 'r-', label='Validation')\n            plt.title('Loss')\n            plt.xlabel('Epoch')\n            plt.grid(True)\n            plt.legend()\n            \n            # MSE\n            plt.subplot(2, 2, 2)\n            plt.plot(self.train_metrics['epoch'], self.train_metrics['mse'], 'b-', label='Train')\n            plt.plot(self.val_metrics['epoch'], self.val_metrics['mse'], 'r-', label='Validation')\n            plt.title('MSE')\n            plt.xlabel('Epoch')\n            plt.grid(True)\n            plt.legend()\n            \n            # RMSE\n            plt.subplot(2, 2, 3)\n            plt.plot(self.train_metrics['epoch'], self.train_metrics['rmse'], 'b-', label='Train')\n            plt.plot(self.val_metrics['epoch'], self.val_metrics['rmse'], 'r-', label='Validation')\n            plt.title('RMSE')\n            plt.xlabel('Epoch')\n            plt.grid(True)\n            plt.legend()\n            \n            # R²\n            plt.subplot(2, 2, 4)\n            plt.plot(self.train_metrics['epoch'], self.train_metrics['r2'], 'b-', label='Train')\n            plt.plot(self.val_metrics['epoch'], self.val_metrics['r2'], 'r-', label='Validation')\n            plt.title('R² Score')\n            plt.xlabel('Epoch')\n            plt.grid(True)\n            plt.legend()\n            \n            plt.tight_layout()\n            plt.savefig('final_metrics_plot.png', dpi=300)\n            plt.show()\n            \n        except Exception as e:\n            print(f\"Erreur lors du tracé: {str(e)}\")\n\n    def plot_variable_metrics(self):\n        \"\"\"Trace les métriques pour chaque variable séparément\"\"\"\n        fig, axs = plt.subplots(4, 4, figsize=(20, 15))\n        \n        for i, var_name in enumerate(self.variable_names):\n            # MSE\n            axs[0, i].plot(self.train_var_metrics[var_name]['mse'], 'b-', label=\"Train\")\n            axs[0, i].plot(self.val_var_metrics[var_name]['mse'], 'r-', label=\"Val\")\n            axs[0, i].set_title(f\"{var_name} - MSE\")\n            axs[0, i].grid()\n            \n            # RMSE\n            axs[1, i].plot(self.train_var_metrics[var_name]['rmse'], 'b-', label=\"Train\")\n            axs[1, i].plot(self.val_var_metrics[var_name]['rmse'], 'r-', label=\"Val\")\n            axs[1, i].set_title(f\"{var_name} - RMSE\")\n            axs[1, i].grid()\n            \n            # MAE\n            axs[2, i].plot(self.train_var_metrics[var_name]['mae'], 'b-', label=\"Train\")\n            axs[2, i].plot(self.val_var_metrics[var_name]['mae'], 'r-', label=\"Val\")\n            axs[2, i].set_title(f\"{var_name} - MAE\")\n            axs[2, i].grid()\n            \n            # R2\n            axs[3, i].plot(self.train_var_metrics[var_name]['r2'], 'b-', label=\"Train\")\n            axs[3, i].plot(self.val_var_metrics[var_name]['r2'], 'r-', label=\"Val\")\n            axs[3, i].set_title(f\"{var_name} - R²\")\n            axs[3, i].grid()\n        \n        plt.tight_layout()\n        plt.savefig(\"variable_metrics.png\")\n        plt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------\n# 4. Entraînement et évaluation\n# -------------------------------------------------\nmodel = TrafficModel()\n\ntrainer = pl.Trainer(\n    max_epochs=EPOCHS,\n    accelerator=\"auto\",\n    devices=\"auto\",\n    enable_progress_bar=True,\n    deterministic=False,\n    callbacks=[\n        pl.callbacks.EarlyStopping(\n            monitor=\"val_loss\",\n            patience=5,\n            mode=\"min\"\n        ),\n        pl.callbacks.ModelCheckpoint(\n            monitor=\"val_loss\",\n            filename=\"best-checkpoint\",\n            save_top_k=1,\n            mode=\"min\"\n        ),\n        pl.callbacks.LearningRateMonitor()\n    ],\n    log_every_n_steps=10\n)\n\n# Entraînement\ntry:\n    print(\"Début de l'entraînement...\")\n    trainer.fit(model, train_loader, val_loader)\n    \n    # Test du meilleur modèle\n    print(\"\\nÉvaluation sur le test set...\")\n    best_model = TrafficModel.load_from_checkpoint(\n        trainer.checkpoint_callback.best_model_path\n    )\n    \n    \n    # Sauvegarde des métriques dans un DataFrame\n    train_df = pd.DataFrame(best_model.train_metrics)\n    val_df = pd.DataFrame(best_model.val_metrics)\n    test_df = pd.DataFrame(best_model.test_metrics)\n    \n    # Affichage des métriques finales\n    print(\"\\nRésumé des métriques finales:\")\n    print(\"\\nEntraînement:\")\n    print(train_df.tail())\n    print(\"\\nValidation:\")\n    print(val_df.tail())\n    print(\"\\nTest:\")\n    print(test_df)\n    \nexcept Exception as e:\n    print(f\"\\nErreur pendant l'entraînement : {str(e)}\")\n    raise e","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -- 9. Visualisation ( étendue à toutes les cibles) --\nprint(\"Generating visualizations for all target variables...\")\n\n# 1. Courbes d'apprentissage (Loss)\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nbest_epoch_marker = len(val_losses) - epochs_no_improve - 1 if best_model_state and epochs_no_improve < len(val_losses) else len(val_losses) - 1\nplt.axvline(x=best_epoch_marker, color='r', linestyle='--', label=f'Best Epoch ({best_epoch_marker+1})')\nplt.title('Model2 Training and Validation Loss (GAT Version)')  # Mise à jour du titre\nplt.xlabel('Epochs')\nplt.ylabel('Huber Loss')\nplt.legend()\nplt.grid(True)\nplt.savefig('training_validation_loss_gat.png')\nprint(\"Saved: training_validation_loss_gat.png\")\nplt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"A = model.plot_weighted_graph(top_k=421)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Epochs train:\", len(model.train_metrics['epoch']))\nprint(\"Loss train:\", len(model.train_metrics['loss']))\nprint(\"Epochs val:\", len(model.val_metrics['epoch']))\nprint(\"Loss val:\", len(model.val_metrics['loss']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test final\nresults = trainer.test(model, test_loader)\nprint(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.plot_metrics()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Passer l'ensemble de test dans le modèle\nmodel.eval()  # Mettre le modèle en mode évaluation\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# Test loop\nwith torch.no_grad():\n    predictions = []\n    true_values = []\n    for batch in test_loader:\n        x, y = batch\n        x, y = x.to(device), y.to(device)  # Déplacer les données sur le même appareil que le modèle\n        y_hat = model(x)\n        predictions.append(y_hat)\n        true_values.append(y)\n\n# Concaténer les résultats\npredictions = torch.cat(predictions, dim=0)\ntrue_values = torch.cat(true_values, dim=0)\n\n# Afficher les résultats\nprint(\"Predictions:\", predictions)\nprint(\"True values:\", true_values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.dates as mdates\nfrom datetime import datetime, timedelta\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n\n# Vérification des shapes\nprint(f\"Shape de true_values: {true_values.shape}\")\nprint(f\"Shape de predictions: {predictions.shape}\")\n\n# Conversion des tenseurs si nécessaire\nif hasattr(true_values, 'cpu'):  # Si c'est un tenseur PyTorch\n    true_values = true_values.cpu().numpy()\nif hasattr(predictions, 'cpu'):\n    predictions = predictions.cpu().numpy()\n\n# Redimensionnement des prédictions si nécessaire (supprimer la dimension inutile)\nif predictions.ndim == 4 and predictions.shape[2] == 1:  # Si shape [1170, 421, 1, 4]\n    predictions = predictions.squeeze(2)  # Devient [1170, 421, 4]\n\n# Définir le jour spécifique à visualiser\nspecific_day = datetime(2021, 9, 2)  # Adaptez cette date\n\n# Créer les dates\nstart_date = datetime(2021, 6, 1)  # Date de début des données\ndates = [start_date + timedelta(hours=2*i) for i in range(true_values.shape[0])]\n\n# Filtrer les indices du jour spécifique\nday_indices = [i for i, date in enumerate(dates) if date.date() == specific_day.date()]\n\n# Vérifier qu'on a des données pour ce jour\nif not day_indices:\n    raise ValueError(f\"Aucune donnée disponible pour le {specific_day.strftime('%d/%m/%Y')}\")\n\n# Choisir un nœud spécifique\nnode_idx = 10  # À adapter\nfeature_names = [\"Volume\", \"Occupation\", \"Niveau de congestion\", \"Vitesse moyenne\"]\n\n# Création de la figure\nfig, axs = plt.subplots(2, 2, figsize=(15, 12))\naxs = axs.flatten()\ncolors = ['blue', 'green', 'purple', 'brown']\n\n# Tracer chaque caractéristique\nfor feature_idx in range(4):\n    ax = axs[feature_idx]\n    \n    # Extraction des données\n    true_day_values = true_values[day_indices, node_idx, feature_idx]\n    pred_day_values = predictions[day_indices, node_idx, feature_idx]\n    \n    # Calcul des métriques pour ce jour/capteur/feature\n    mae = mean_absolute_error(true_day_values, pred_day_values)\n    rmse = np.sqrt(mean_squared_error(true_day_values, pred_day_values))\n    r2 = r2_score(true_day_values, pred_day_values)\n    \n    # Tracé\n    day_dates = [dates[i] for i in day_indices]\n    ax.plot(day_dates, true_day_values, label=\"Valeurs réelles\", \n            linewidth=2, color=colors[feature_idx])\n    ax.plot(day_dates, pred_day_values, label=\"Prédictions\", \n            linewidth=2, linestyle='--', color=colors[feature_idx])\n    \n    # Configuration des axes\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n    ax.xaxis.set_major_locator(mdates.HourLocator(interval=2))\n    ax.grid(True, linestyle='--', alpha=0.7)\n    ax.legend()\n    ax.set_xlabel(\"Heure\")\n    ax.set_ylabel(feature_names[feature_idx])\n    ax.set_title(f\"{feature_names[feature_idx]} - Capteur {node_idx}\\n\"\n                f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.2f}\")\n\n# Titre global\nplt.suptitle(f\"Prédictions vs Valeurs réelles - {specific_day.strftime('%d/%m/%Y')}\", \n             fontsize=16, fontweight='bold')\n\n# Ajustement et sauvegarde\nplt.tight_layout()\nplt.savefig(f\"prediction_sensor_{node_idx}_day_{specific_day.strftime('%Y%m%d')}.png\", \n            dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Récupérer la matrice d'adjacence après entraînement de MTGNN\n# Après entraînement\nadj = model.get_adjacency_matrix()\nprint(\"Matrice shape:\", adj.shape)\nprint(adj)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau # Importation de l'ordonnanceur\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch_geometric.nn import GATConv # Remplacement de GCNConv par GATConv\nfrom sklearn.preprocessing import StandardScaler # Remplacement de MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.neighbors import BallTree\nimport math\nimport matplotlib.pyplot as plt\nimport time\nimport copy\n\n\n\n# Paramètres du modèle et de l'entraînement (ajustés/ajoutés)\nnum_nodes = 421\nNUM_FEATURES = 11\nSEQ_LEN = 12\nHORIZON = 1\nBATCH_SIZE = 32\nEPOCHS = 20 # Augmenter potentiellement, l'early stopping gère la fin\nLEARNING_RATE = 0.001 # Point de départ, à tuner !\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nHIDDEN_DIM = 64\nGAT_LAYERS = 2      # Nombre de couches GAT\nGAT_HEADS = 4       # Nombre de têtes d'attention pour GATConv (output dim = HIDDEN_DIM * GAT_HEADS si concat, ou HIDDEN_DIM si moyenné)\nGRU_LAYERS = 1\nDROPOUT_RATE = 0.3\nnum_targets=4\n\n# Early Stopping\nEARLY_STOPPING_PATIENCE = 5 # Légèrement augmenté\nEARLY_STOPPING_DELTA = 0.0\n\n# Paramètres pour la construction du graphe\nDISTANCE_THRESHOLD_KM = 1.0\nADD_SELF_LOOPS = True # Important pour GAT aussi\nEDGE_WEIGHT_SIGMA = 0.5 # Conservé pour info, mais GAT apprendra ses propres poids\n\nprint(f\"Using device: {DEVICE}\")\n\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371\n    dLat = math.radians(lat2 - lat1)\n    dLon = math.radians(lon2 - lon1)\n    lat1, lat2 = map(math.radians, [lat1, lat2])\n    a = math.sin(dLat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dLon / 2)**2\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    return R * c\n\ndef build_spatial_graph_for_gat(sensor_locations_df, id_col, lat_col, lon_col, distance_threshold_km, add_self_loops=True):\n    \"\"\"\n    Construit l'index des arêtes pour GAT (basé sur la proximité).\n    Retourne adj_matrix (binaire ou pondérée pour info) et edge_index.\n    \"\"\"\n    locations = sensor_locations_df[[id_col, lat_col, lon_col]].drop_duplicates(subset=[id_col]).set_index(id_col)\n    sensor_ids = locations.index.tolist()\n    num_nodes = len(sensor_ids)\n    sensor_id_to_index = {sensor_id: i for i, sensor_id in enumerate(sensor_ids)}\n\n    print(f\"Building graph connectivity for {num_nodes} sensors...\")\n    coords_rad = np.radians(locations[[lat_col, lon_col]].values)\n    tree = BallTree(coords_rad, metric='haversine')\n    dist_threshold_rad = distance_threshold_km / 6371\n\n    adj_matrix = np.zeros((num_nodes, num_nodes), dtype=np.float32) # Pour info ou debug\n    edge_list = []\n\n    # Correction ici: supprimer le '[0]' à la fin car return_distance=False par défaut\n    indices = tree.query_radius(coords_rad, r=dist_threshold_rad)\n\n    # Maintenant 'indices' est un tableau où chaque élément est un tableau d'indices de voisins\n    for i, neighbors_indices in enumerate(indices):\n        # neighbors_indices est maintenant un tableau (itérable)\n        for j in neighbors_indices:\n            if i == j: continue\n            adj_matrix[i, j] = 1 # Matrice binaire simple\n            adj_matrix[j, i] = 1 # Symétrique\n            # Ajouter la paire (i,j) si elle n'est pas déjà dans edge_list pour éviter doublons GAT\n            # (Bien que set() à la fin gère cela aussi)\n            if (i,j) not in edge_list and (j,i) not in edge_list:\n                 edge_list.append((i, j))\n\n\n    # Gérer les self-loops pour edge_index si GATConv ne le fait pas (souvent mieux de les inclure)\n    if add_self_loops:\n        np.fill_diagonal(adj_matrix, 1.0)\n        for i in range(num_nodes):\n             # Ajouter (i,i) à edge_list si pas déjà présent (pour être sûr)\n             if not any(item == (i, i) for item in edge_list):\n                 edge_list.append((i, i))\n\n    # Convertir en tensor (set() n'est plus nécessaire si on gère les doublons plus haut)\n    # S'assurer qu'il n'y a pas de doublons si on n'a pas géré plus haut\n    # edge_list = list(set(edge_list)) # Optionnel si la logique ci-dessus est bonne\n   \n    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n\n    print(f\"Graph connectivity built with {edge_index.shape[1]} edges (including self-loops if added).\")\n    return adj_matrix, edge_index, sensor_id_to_index\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# -- Graph Construction --\nadj_matrix, edge_index, sensor_id_to_index = build_spatial_graph_for_gat(\n    unique_combinations, 'id','lon','lat', DISTANCE_THRESHOLD_KM, ADD_SELF_LOOPS\n)\n\n# Move edge_index to the correct device\nedge_index = edge_index.to(DEVICE) \ndata_tensor = torch.tensor(data_restructured_final, dtype=torch.float32)\n\ndef create_sequences(data, SEQ_LEN, horizon):\n    sequences = []\n    targets = []\n    for i in range(len(data) - SEQ_LEN - horizon):\n        seq = data[i:i+SEQ_LEN]  # (seq_len, num_nodes, num_features)\n        target = data[i+SEQ_LEN, :, :4]  # (num_nodes, 4)\n        sequences.append(seq)\n        targets.append(target)\n    return torch.stack(sequences), torch.stack(targets)\n\nsequences, targets = create_sequences(data_tensor, SEQ_LEN, HORIZON)\n\ndef temporal_split(sequences, targets, train_ratio=0.8, val_ratio=0.1):\n    num_samples = len(sequences)\n    train_end = int(train_ratio * num_samples)\n    val_end = train_end + int(val_ratio * num_samples)\n    \n    # Découpage séquentiel\n    train_seq, train_targ = sequences[:train_end], targets[:train_end]\n    val_seq, val_targ = sequences[train_end:val_end], targets[train_end:val_end]\n    test_seq, test_targ = sequences[val_end:], targets[val_end:]\n    \n    return train_seq, val_seq, test_seq, train_targ, val_targ, test_targ\n\n# Application\ntrain_seq, val_seq, test_seq, train_targ, val_targ, test_targ = temporal_split(sequences, targets)\n\n# -------------------------------------------------\n# 2. Dataset et DataLoader\n# -------------------------------------------------\nclass TrafficDataset(Dataset):\n    def __init__(self, sequences, targets):\n        self.sequences = sequences\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        return self.sequences[idx], self.targets[idx]\n\n# Création des datasets\ntrain_dataset = TrafficDataset(train_seq, train_targ)\nval_dataset = TrafficDataset(val_seq, val_targ)\ntest_dataset = TrafficDataset(test_seq, test_targ)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,num_workers=4, persistent_workers=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4, persistent_workers=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4, persistent_workers=True)\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -- 6. Définition du Modèle ST-GNN (avec GAT) --\nclass STGNN_GAT(nn.Module):\n    def __init__(self, num_nodes, num_features, num_targets, seq_len, horizon, hidden_dim, gat_layers=1, gat_heads=1, gru_layers=1, dropout_rate=0.0):\n        super(STGNN_GAT, self).__init__()\n        self.num_nodes = num_nodes\n        self.num_targets = num_targets\n        self.horizon = horizon\n        self.hidden_dim = hidden_dim\n        self.gat_heads = gat_heads\n\n        # Couche GRU initiale\n        self.gru1 = nn.GRU(num_features, hidden_dim, gru_layers, batch_first=True)\n        self.ln1 = nn.LayerNorm([num_nodes, hidden_dim])\n\n        # Couches GAT\n        self.gat_layers = nn.ModuleList()\n        self.gat_lns = nn.ModuleList()\n        # La première couche GAT prend hidden_dim en entrée\n        self.gat_layers.append(GATConv(hidden_dim, hidden_dim, heads=gat_heads, dropout=dropout_rate))\n        self.gat_lns.append(nn.LayerNorm([num_nodes, hidden_dim * gat_heads])) # La sortie est multipliée par le nb de têtes\n       \n        # Les couches GAT suivantes prennent hidden_dim * gat_heads en entrée\n        for _ in range(1, gat_layers):\n            self.gat_layers.append(GATConv(hidden_dim * gat_heads, hidden_dim, heads=gat_heads, dropout=dropout_rate))\n            self.gat_lns.append(nn.LayerNorm([num_nodes, hidden_dim * gat_heads]))\n\n        self.relu = nn.ELU() # ELU est souvent utilisé avec GAT\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n        # Couche de sortie : l'entrée dépend de la sortie de la dernière couche GAT\n        # Si on moyenne les têtes sur la dernière couche, l'entrée est hidden_dim\n        # Si on concatène, c'est hidden_dim * gat_heads. Simplifions en moyennant implicitement ou via une couche linéaire.\n        # Ou ajustons la dernière couche GAT pour sortir hidden_dim directement en moyennant.\n        # Ici, on ajoute une couche linéaire pour mapper la sortie GAT (concaténée) vers la dim de sortie voulue.\n        # Si la dernière couche GAT moyenne les têtes, utiliser : self.fc = nn.Linear(hidden_dim, horizon * num_targets)\n        self.fc = nn.Linear(hidden_dim * gat_heads, horizon * num_targets) # Si la sortie GAT est concaténée\n\n    def forward(self, x, edge_index): # Ne prend plus edge_weight\n        # x shape: [batch_size, seq_len, num_nodes, num_features]\n        batch_size, seq_len, _, _ = x.shape\n\n        # Traitement temporel initial\n        x_reshaped = x.permute(0, 2, 1, 3).reshape(batch_size * self.num_nodes, seq_len, -1)\n        out, _ = self.gru1(x_reshaped)\n        gru1_out = out[:, -1, :].reshape(batch_size, self.num_nodes, self.hidden_dim)\n        gru1_out_norm = self.ln1(gru1_out)\n\n        # Traitement spatial avec GAT\n        gat_input = gru1_out_norm # [batch_size, num_nodes, hidden_dim]\n       \n        hidden_gat_list = []\n        # Itérer sur le batch car GATConv standard traite un graphe à la fois\n        for i in range(batch_size):\n            hidden_batch = gat_input[i] # [num_nodes, hidden_dim]\n            for k, layer in enumerate(self.gat_layers):\n                # GATConv attend (x, edge_index)\n                hidden_batch = self.relu(layer(hidden_batch, edge_index))\n                # Appliquer LayerNorm après chaque GAT\n                hidden_batch = self.gat_lns[k](hidden_batch)\n            hidden_gat_list.append(hidden_batch)\n       \n        final_features = torch.stack(hidden_gat_list) # [batch_size, num_nodes, hidden_dim * gat_heads]\n\n        final_features_dropout = self.dropout(final_features)\n\n        # Couche de sortie\n        output = self.fc(final_features_dropout) # [batch_size, num_nodes, horizon * num_targets]\n\n        return output\n\n\n# Instanciation du modèle\nmodel2 = STGNN_GAT(num_nodes=num_nodes,\n                  num_features=num_features,\n                  num_targets=num_targets,\n                  seq_len=SEQ_LEN,\n                  horizon=HORIZON,\n                  hidden_dim=HIDDEN_DIM,\n                  gat_layers=GAT_LAYERS,\n                  gat_heads=GAT_HEADS,\n                  gru_layers=GRU_LAYERS,\n                  dropout_rate=DROPOUT_RATE).to(DEVICE)\n\nprint(\"Model Architecture:\")\nprint(model2)\nprint(f\"Total Parameters: {sum(p.numel() for p in model2.parameters() if p.requires_grad)}\")\n\n# -- 7. Entraînement du Modèle (avec HuberLoss et LR Scheduler) --\ncriterion = nn.HuberLoss() # Changement de fonction de perte\noptimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)\nscheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True) # Ordonnanceur\n\nprint(\"Starting training with HuberLoss, LR Scheduler, and Early Stopping...\")\ntrain_losses = []\nval_losses = []\nbest_val_loss = float('inf')\nepochs_no_improve = 0\nbest_model_state = None\n\nfor epoch in range(EPOCHS):\n    epoch_start_time = time.time()\n    model2.train()\n    running_train_loss = 0.0\n    for batch_x, batch_y in train_loader:\n        batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model2(batch_x, edge_index) # Ne passe plus edge_weight\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        running_train_loss += loss.item()\n    avg_train_loss = running_train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    # Validation\n    model2.eval()\n    running_val_loss = 0.0\n    with torch.no_grad():\n        for batch_x_val, batch_y_val in val_loader:\n            batch_x_val, batch_y_val = batch_x_val.to(DEVICE), batch_y_val.to(DEVICE)\n            outputs_val = model2(batch_x_val, edge_index) # Ne passe plus edge_weight\n            loss_val = criterion(outputs_val, batch_y_val)\n            running_val_loss += loss_val.item()\n    avg_val_loss = running_val_loss / len(val_loader)\n    val_losses.append(avg_val_loss)\n\n    epoch_end_time = time.time()\n    current_lr = optimizer.param_groups[0]['lr']\n    print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}, LR: {current_lr:.6f}, Time: {epoch_end_time - epoch_start_time:.2f}s\")\n\n    # Mise à jour de l'ordonnanceur basé sur la perte de validation\n    scheduler.step(avg_val_loss)\n\n    # Early Stopping Check\n    if avg_val_loss < best_val_loss - EARLY_STOPPING_DELTA:\n        best_val_loss = avg_val_loss\n        epochs_no_improve = 0\n        best_model_state = copy.deepcopy(model2.state_dict())\n        print(f\"  Validation loss improved to {best_val_loss:.6f}. Saving model state.\")\n    else:\n        epochs_no_improve += 1\n        print(f\"  Validation loss did not improve for {epochs_no_improve} epoch(s).\")\n\n    if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n        print(f\"\\nEarly stopping triggered after {epoch + 1} epochs.\")\n        break\n\n# Charger le meilleur modèle\nif best_model_state:\n    print(\"\\nLoading best model state found during training.\")\n    model2.load_state_dict(best_model_state)\nelse:\n     print(\"\\nWarning: No best model state saved. Using last state.\")\n\nprint(\"Training finished.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -- 8. Évaluation sur l'Ensemble de Test --\nprint(\"Evaluating on test set using the best model...\")\nmodel2.eval()  # On utilise model2 partout maintenant\n\n# Déplacer edge_index sur le bon device\nedge_index = edge_index.to(DEVICE)\n\npredictions = []\nactuals = []\n\nwith torch.no_grad():\n    for batch_x_test, batch_y_test in test_loader:\n        batch_x_test, batch_y_test = batch_x_test.to(DEVICE), batch_y_test.to(DEVICE)\n        outputs_test = model2(batch_x_test, edge_index)  # Changé model -> model2\n        \n        # Conversion en numpy et stockage\n        predictions.append(outputs_test.cpu().numpy())\n        actuals.append(batch_y_test.cpu().numpy())\n\n# Concaténation des résultats\npredictions = np.concatenate(predictions, axis=0)\nactuals = np.concatenate(actuals, axis=0)\n\n# Redimensionnement pour conserver la structure [samples, nodes, horizon, targets]\npredictions_final = predictions.reshape(-1, num_nodes, HORIZON, num_targets)\nactuals_final = actuals.reshape(-1, num_nodes, HORIZON, num_targets)\n\n# Calcul des métriques\nprint(\"\\n--- Test Set Evaluation Metrics (Standardized Scale) ---\")\nTARGET_COLS = ['volume', 'occupation', 'congestion_level', 'speed_avg']  # Assurez-vous que c'est défini\n\nfor i, target_name in enumerate(TARGET_COLS):\n    pred = predictions_final[:, :, 0, i].flatten()\n    true = actuals_final[:, :, 0, i].flatten()\n    \n    # Filtrage des valeurs non finies\n    mask = np.isfinite(true) & np.isfinite(pred)\n    true = true[mask]\n    pred = pred[mask]\n    \n    if len(true) < 2:\n        print(f\"{target_name}: Not enough valid samples\")\n        continue\n    \n    # Calcul des métriques\n    mse = mean_squared_error(true, pred)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(true - pred))\n    r2 = r2_score(true, pred)\n    \n    print(f\"{target_name}:\")\n    print(f\"  MSE: {mse:.4f}, RMSE: {rmse:.4f}\")\n    print(f\"  MAE: {mae:.4f}, R²: {r2:.4f}\")\n    print(\"-\"*40)\n\n# -- 9. Visualisation ( étendue à toutes les cibles) --\nprint(\"Generating visualizations for all target variables...\")\n\n# 1. Courbes d'apprentissage (Loss)\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nbest_epoch_marker = len(val_losses) - epochs_no_improve - 1 if best_model_state and epochs_no_improve < len(val_losses) else len(val_losses) - 1\nplt.axvline(x=best_epoch_marker, color='r', linestyle='--', label=f'Best Epoch ({best_epoch_marker+1})')\nplt.title('Model2 Training and Validation Loss (GAT Version)')  # Mise à jour du titre\nplt.xlabel('Epochs')\nplt.ylabel('Huber Loss')\nplt.legend()\nplt.grid(True)\nplt.savefig('training_validation_loss_gat.png')\nprint(\"Saved: training_validation_loss_gat.png\")\nplt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Choisir un capteur à visualiser (peut être modifié)\nsensor_index_to_plot = 0\nsensor_id_plotted = unique_combinations['id'][sensor_index_to_plot]\nprint(f\"\\nGenerating comparison plots for Sensor ID: {sensor_id_plotted} (Index: {sensor_index_to_plot})\")\n\nfor i, target_name in enumerate(TARGET_COLS):\n    print(f\"  Generating plots for target: {target_name}\")\n\n    # 2. Comparaison Actuel vs. Prédit (Série Temporelle) pour ce capteur et cette cible\n    actual_sensor = actuals_final[:, sensor_index_to_plot, 0, i]\n    predicted_sensor = predictions_final[:, sensor_index_to_plot, 0, i]\n\n    plt.figure(figsize=(15, 6))\n    plt.plot(actual_sensor, label='Actual', color='blue', marker='.', markersize=4, linestyle='')\n    plt.plot(predicted_sensor, label='Predicted', color='red', alpha=0.7)\n    plt.title(f'Actual vs Predicted {target_name} for Sensor {sensor_id_plotted} (Test Set - GAT Model)')\n    plt.xlabel('Time Steps (in test set)')\n    plt.ylabel(f'{target_name}') # L'échelle est celle des données originales grâce à inverse_transform\n    plt.legend()\n    plt.grid(True)\n    filename_ts = f'actual_vs_predicted_{target_name}_sensor_{sensor_id_plotted}_gat.png'\n    plt.savefig(filename_ts)\n    print(f\"    Saved: {filename_ts}\")\n    # plt.show() # Décommentez pour afficher chaque graphique immédiatement\n    plt.close() # Ferme la figure\n\n    # 3. Scatter plot Prédit vs Actuel pour cette cible (tous les nœuds)\n    plt.figure(figsize=(8, 8))\n    actual_flat = actuals_final[:, :, 0, i].flatten()\n    pred_flat = predictions_final[:, :, 0, i].flatten()\n    mask = np.isfinite(actual_flat) & np.isfinite(pred_flat) # Utiliser seulement les points finis\n\n    if np.sum(mask) > 0: # Vérifier s'il y a des points valides à tracer\n        plt.scatter(actual_flat[mask], pred_flat[mask], alpha=0.2, s=5)\n        plt.xlabel(f'Actual {target_name}')\n        plt.ylabel(f'Predicted {target_name}')\n        plt.title(f'Predicted vs Actual {target_name} (All nodes, Test Set - GAT Model)')\n        # Ligne y=x pour référence\n        min_val = min(np.min(actual_flat[mask]), np.min(pred_flat[mask]))\n        max_val = max(np.max(actual_flat[mask]), np.max(pred_flat[mask]))\n        plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--')\n        plt.grid(True)\n        plt.axis('equal') # Assure une échelle égale sur les deux axes\n        filename_scatter = f'scatter_predicted_vs_actual_{target_name}_gat.png'\n        plt.savefig(filename_scatter)\n        print(f\"    Saved: {filename_scatter}\")\n    else:\n        print(f\"    Skipped scatter plot for {target_name}: No valid data points.\")\n\n    # plt.show() # Décommentez pour afficher chaque graphique immédiatement\n    plt.close() # Ferme la figure\n\nprint(\"\\nAll visualizations generated and saved as PNG files.\")\nprint(\"Code execution finished.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.dates as mdates\nfrom datetime import datetime, timedelta\n\n# Utilisez les tableaux redimensionnés (4D) pour la visualisation\npredictions = predictions_final\nactuals = actuals_final\n\n# Définir le jour spécifique à visualiser\nspecific_day = datetime(2021, 9, 2)  # Adaptez à votre cas\n\n# Créer les dates\nstart_date = datetime(2021, 6, 1)  # Date de début de vos données\ndates = [start_date + timedelta(hours=2*i) for i in range(actuals.shape[0])]\n\n# Filtrer les indices du jour spécifique\nday_indices = [i for i, date in enumerate(dates) if date.date() == specific_day.date()]\n\n# Choisir un nœud spécifique\nnode_idx = 10 # À adapter\n\n# Noms des caractéristiques\nfeature_names = [\"Volume\", \"Occupation\", \"Niveau de congestion\", \"Vitesse moyenne\"]\n\n# Création de la figure\nfig, axs = plt.subplots(2, 2, figsize=(15, 12))\naxs = axs.flatten()\n\n# Couleurs pour les courbes\ncolors = ['blue', 'green', 'purple', 'brown']\n\n# Tracer chaque caractéristique\nfor feature_idx in range(4):\n    ax = axs[feature_idx]\n    \n    # Extraction des données depuis les tableaux 4D\n    true_day_values = actuals[day_indices, node_idx, 0, feature_idx]  # [samples, nodes, horizon, features]\n    pred_day_values = predictions[day_indices, node_idx, 0, feature_idx]\n    \n    # Tracé\n    ax.plot(dates[day_indices[0]:day_indices[-1]+1], true_day_values, \n            label=\"Valeurs réelles\", linewidth=2, color=colors[feature_idx])\n    ax.plot(dates[day_indices[0]:day_indices[-1]+1], pred_day_values, \n            label=\"Prédictions\", linewidth=2, linestyle='--', color=colors[feature_idx])\n    \n    # Configuration des axes\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n    ax.xaxis.set_major_locator(mdates.HourLocator(interval=2))\n    ax.grid(True, linestyle='--', alpha=0.7)\n    ax.legend()\n    ax.set_xlabel(\"Heure\")\n    ax.set_ylabel(feature_names[feature_idx])\n    ax.set_title(f\"{feature_names[feature_idx]} - Capteur {node_idx}\")\n\n# Titre global\nplt.suptitle(f\"Prédictions vs Valeurs réelles - {specific_day.strftime('%d/%m/%Y')}\", \n             fontsize=16, fontweight='bold')\n\n# Ajustement et sauvegarde\nplt.tight_layout()\nplt.savefig(f\"prediction_sensor_{node_idx}_day_{specific_day.strftime('%Y%m%d')}.png\", \n            dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import datetime, timedelta\nimport numpy as np\n\n# Supposons que vous avez ces données (à adapter selon vos besoins)\n# true_values: valeurs réelles pour un nœud et une feature spécifique\n# predictions_final, predictions, predictions1: prédictions des 3 modèles\n\n# Créer des dates pour l'axe x (exemple pour 24 heures avec un pas de 2h)\ndates = [datetime(2021, 6, 1) + timedelta(hours=2 * i) for i in range(12)]  # 12 points pour 24h\n\n# Sélectionner un nœud et une feature spécifique\nnode_idx = 0  # Premier capteur\nfeature_idx = 0  # Première caractéristique (par exemple volume)\n\n# Convertir les prédictions en numpy array si ce ne sont pas déjà des arrays\npredictions_final = np.array(predictions_final)\npredictions = np.array(predictions)\npredictions1 = np.array(predictions1)\n\n# Extraire les prédictions pour le nœud et la feature sélectionnés\npred_model_1 = predictions_final[:, node_idx, feature_idx]  # STGNN\npred_model_2 = predictions[:, node_idx, feature_idx]        # MTGNN\npred_model_3 = predictions1[:, node_idx, feature_idx]       # ASTGCN\n\n# Créer une figure\nplt.figure(figsize=(12, 6))\n\n# Tracer les valeurs réelles\nplt.plot(dates, true_values, label=\"Valeurs réelles\", linewidth=2, color=\"black\")\n\n# Tracer les prédictions des 3 modèles\nplt.plot(dates, pred_model_1, label=\"STGNN\", linestyle=\"--\", color=\"blue\")\nplt.plot(dates, pred_model_2, label=\"MTGNN\", linestyle=\"--\", color=\"green\")\nplt.plot(dates, pred_model_3, label=\"ASTGCN\", linestyle=\"--\", color=\"red\")\n\n# Formater l'axe des x pour afficher les heures\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\nplt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=2))\n\n# Ajouter légendes, titres, etc.\nplt.xlabel(\"Heure\", fontsize=12)\nplt.ylabel(\"Valeur (Volume, Vitesse, etc.)\", fontsize=12)\nplt.title(f\"Comparaison des prédictions pour le capteur {node_idx} - Caractéristique {feature_idx}\", fontsize=14)\nplt.legend(loc='upper right')\n\n# Ajouter une grille\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Ajuster la mise en page\nplt.tight_layout()\n\n# Afficher le graphique\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}